{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data import get_dataset\n",
    "from utils.training_utils import get_lr, get_run_name, AverageMeter\n",
    "from torch.utils.data import DataLoader\n",
    "from models import get_model\n",
    "from tokenizing import get_tokenizer\n",
    "import wandb\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from copy import deepcopy\n",
    "from transformers import TrainingArguments\n",
    "from trl import DPOTrainer\n",
    "from transformers import AutoTokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Next-token failures\")\n",
    "# Data\n",
    "parser.add_argument(\n",
    "    \"--n_samples\", type=int, default=5, help=\"Number of samples to generate\"\n",
    "    )\n",
    "parser.add_argument(\n",
    "    \"--model\", default='gpt2', type=str, help=\"Type of model\"\n",
    "    )\n",
    "parser.add_argument(\n",
    "    \"--dataset\", default='graph', type=str, help=\"Choice of dataset\"\n",
    "    )\n",
    "parser.add_argument(\n",
    "    \"--n_train\", default=200000, type=int, help=\"Number of training samples\"\n",
    "    )\n",
    "parser.add_argument(\n",
    "    \"--n_test\", default=500, type=int, help=\"Number of test samples\"\n",
    "    )\n",
    "parser.add_argument(\n",
    "    \"--num_nodes\", default=50, type=int, help=\"Number of node values in graph\"\n",
    "    )\n",
    "parser.add_argument(\n",
    "    \"--deg\", default=2, type=int, help=\"Degree of starting node\"\n",
    "    )\n",
    "parser.add_argument(\n",
    "    \"--path_len\", default=5, type=int, help=\"Path length in star graph\"\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--mate_in\", default=2, type=int, help=\"For chess, number of moves to checkmate\"\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--unrolled\", action=argparse.BooleanOptionalAction, default=True, help=\"For chess, unrolled board state\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--batch_size\", type=int, default=48, help=\"Batch size\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--lr\", type=float, default=1e-5, help=\"Learning rate\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--weight_decay\", type=float, default=0., help=\"Strength of weight decay\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--epochs_sft\", type=int, default=1, help=\"Number of SFT epochs\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--save_every\", type=int, default=5000, help=\"Interval (in steps) at which to save model\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--pass_at_k\", type=int, default=1, help=\"pass at k eval\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--teacherless\", action=argparse.BooleanOptionalAction, default=False, help=\"Standard or teacherless training\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--reverse\", action=argparse.BooleanOptionalAction, default=False, help=\"Standard format or reverse targets\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--cot\", action=argparse.BooleanOptionalAction, default=False, help=\"Standard format or cot targets\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--pos\", action=argparse.BooleanOptionalAction, default=False, help=\"Standard format or pos tokens\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--eval_train\", action=argparse.BooleanOptionalAction, default=False, help=\"Eval for training set\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--eval_every\", type=int, default=400, help=\"Interval (in steps) to evaluate the model on test\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--use_wandb\", action=argparse.BooleanOptionalAction, default=False, help=\"Whether to use wandb\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--wandb_entity\", type=str, default=5000, help=\"Wandb username\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "    \"--pad_length\", default=0, type=int, help=\"Default value for pad length\"\n",
    ")\n",
    "\n",
    "\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System stuff\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "wandb_entity = args.wandb_entity\n",
    "wandb_log = args.use_wandb\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "\n",
    "# Model stuff\n",
    "top_k = 1000\n",
    "temperature = 1.\n",
    "pass_at_k = args.pass_at_k\n",
    "n_samples = args.n_samples\n",
    "\n",
    "# Evaluation stuff\n",
    "eval_iters = 1000\n",
    "eval_interval = 5\n",
    "log_interval = 10\n",
    "\n",
    "# Optimiser\n",
    "dtype = 'bfloat16'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "decay_lr = False\n",
    "args.compile = False if device == 'cuda' else False\n",
    "args.use_flash = True if device == 'cuda' else False\n",
    "warmup_iters = 100\n",
    "min_lr = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal sequence lengths!\n",
      "Equal sequence lengths!\n",
      "Equal sequence lengths!\n",
      "Equal sequence lengths!\n",
      "Equal sequence lengths!\n",
      "500 torch.Size([54]) 100 torch.Size([54])\n",
      "W/o COT (tensor([ 2920,    11,  1485,    91,  1954,    11,  2078,    91,  3510,    11,\n",
      "         2327,    91,  2624,    11,  3682,    91,  2327,    11,  2624,    91,\n",
      "         3682,    11,  1828,    91,  1485,    11,  1954,    91,  3510,    11,\n",
      "         2920,    14,  3510,    11,  1828,    28,  3510,    11,  2327,    11,\n",
      "         2624,    11,  3682,    11,  1828, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256], device='cuda:0'), tensor([   -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "           -1,    -1,    -1,    -1,    -1,  3510,    11,  2327,    11,  2624,\n",
      "           11,  3682,    11,  1828, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256], device='cuda:0')) 49,13|23,28|46,35|32,42|35,32|42,22|13,23|46,49/46,22=46,35,32,42,22<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "With COT (tensor([3559,   11, 2682,   91, 1238,   11,   24,   91, 2780,   11,   23,   91,\n",
      "          24,   11, 2075,   91, 1983,   11, 1238,   91, 3559,   11, 1983,   91,\n",
      "          23,   11, 2598,   91, 2682,   11, 2780,   14, 3559,   11, 2598,   28,\n",
      "        2598,   11,   23,   11, 2780,   11, 2682,   11, 3559,   25, 3559,   11,\n",
      "        2682,   11, 2780,   11,   23,   11], device='cuda:0'), tensor([  -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "          -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "          -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1, 2598,\n",
      "          11,   23,   11, 2780,   11, 2682,   11, 3559,   25, 3559,   11, 2682,\n",
      "          11, 2780,   11,   23,   11, 2598], device='cuda:0')) 43,34|20,9|48,8|9,26|27,20|43,27|8,44|34,48/43,44=44,8,48,34,43:43,34,48,8,\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(args)\n",
    "\n",
    "# train data without cot\n",
    "args.cot = False\n",
    "args.n_train = 500\n",
    "args.n_test = 100\n",
    "args.pad_length = 10\n",
    "train_data, test_data = get_dataset(args, tokenizer, device)\n",
    "train_data.num_target_tokens += train_data.pad_length\n",
    "test_data.num_target_tokens += test_data.pad_length\n",
    "train_data.num_tokens += train_data.pad_length\n",
    "test_data.num_tokens += test_data.pad_length\n",
    "\n",
    "# train data with cot\n",
    "args.cot = True\n",
    "args.n_train = 100\n",
    "args.pad_length = 0\n",
    "train_data_wcot, _ = get_dataset(args, tokenizer, device)\n",
    "train_data_wcot.data_file = train_data_wcot.data_file[-args.n_train:]\n",
    "train_data_wcot.tokenized, train_data_wcot.num_prefix_tokens, train_data_wcot.num_target_tokens = train_data_wcot.tokenizer.tokenize(train_data_wcot.data_file)\n",
    "\n",
    "\n",
    "print(len(train_data), train_data[0][0].shape, len(train_data_wcot), train_data_wcot[0][0].shape)\n",
    "print(\"W/o COT\", train_data[0], tokenizer.decode(train_data[0][0]))\n",
    "print(\"With COT\", train_data_wcot[0], tokenizer.decode(train_data_wcot[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 19, 55, 36, 19, 55, 36, 19, 55)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.num_prefix_tokens, train_data.num_target_tokens, train_data.num_tokens, train_data_wcot.num_prefix_tokens, train_data_wcot.num_target_tokens, train_data_wcot.num_tokens, test_data.num_prefix_tokens, test_data.num_target_tokens, test_data.num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "concatenated_train_data = ConcatDataset([train_data, train_data_wcot])\n",
    "train_loader = DataLoader(concatenated_train_data, batch_size=args.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=args.batch_size, shuffle=True)\n",
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, tokenizer, ctx, num_prefix_tokens, y_tokens, pad_length):\n",
    "    model.eval()\n",
    "    total_acc = AverageMeter()\n",
    "    bar = tqdm(loader)\n",
    "    for x in bar:\n",
    "        original_length = x.shape[1]\n",
    "        y = x[:, -y_tokens:].clone()\n",
    "        x = x[:, :num_prefix_tokens].clone()\n",
    "        with ctx:\n",
    "            y_pred = model.generate(x, max_new_tokens=original_length + pad_length, min_length=original_length + pad_length,\n",
    "                                do_sample=False, attention_mask = torch.ones_like(x), pad_token_id=2)\n",
    "        completely_correct = 0 \n",
    "        for i in range(y.shape[0]):\n",
    "            completely_correct += int(tokenizer.decode(y[i]) in tokenizer.decode(y_pred[i]))\n",
    "        completely_correct /= x.shape[0]\n",
    "        total_acc.update(completely_correct, x.shape[0])\n",
    "        bar.set_description(f' accuracy: {total_acc.get(percentage=True):.2f}')\n",
    "    loader.dataset.train()\n",
    "    model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_score(model, loader, tokenizer, ctx, temperature, top_p, n_samples, num_prefix_tokens, num_target_tokens, y_tokens):\n",
    "    model.eval()\n",
    "    bar = tqdm(loader)\n",
    "    x_dataset = []\n",
    "    y_pred_dataset = []\n",
    "    scores_dataset = []\n",
    "    for x in bar:\n",
    "        original_length = x.shape[1]\n",
    "        y = x[:, -y_tokens:].clone()\n",
    "        x = x[:, :num_prefix_tokens].clone()\n",
    "        y = y\n",
    "        y_pred = []\n",
    "        scores = []\n",
    "        for i in range(n_samples):\n",
    "            with ctx:\n",
    "                # print(x.shape, num_target_tokens)\n",
    "                _y_pred = model.generate(x, min_length=num_prefix_tokens+num_target_tokens, \n",
    "                                   max_length=num_prefix_tokens+num_target_tokens, temperature=temperature, top_p=top_p,\n",
    "                                   do_sample=True, attention_mask = torch.ones_like(x), pad_token_id=2)\n",
    "            _scores = []\n",
    "            for i in range(y.shape[0]):\n",
    "                _scores.append(int(tokenizer.decode(y[i]) in tokenizer.decode(_y_pred[i])))\n",
    "            # print(_y_pred.shape)\n",
    "            y_pred.append(_y_pred.cpu())\n",
    "            scores.append(_scores)\n",
    "            \n",
    "        scores = torch.tensor(scores).float().transpose(0, 1)\n",
    "        y_pred = torch.stack(y_pred, dim=0).transpose(0, 1)\n",
    "        # y_pred should be bs x nsamples x length\n",
    "        y_pred_dataset.append(y_pred[:, :, num_prefix_tokens:].cpu())\n",
    "        scores_dataset.append(scores.cpu())\n",
    "        # print(scores.shape, y_pred.shape, x.shape)\n",
    "        x_dataset.append(x.cpu())\n",
    "        # correct should be bs x nsamples\n",
    "    # Switch back to train mode\n",
    "    loader.dataset.train()\n",
    "    model.train()\n",
    "    y_pred_dataset = torch.cat(y_pred_dataset, dim=0)\n",
    "    x_dataset = torch.cat(x_dataset, dim=0)\n",
    "    scores_dataset = torch.cat(scores_dataset, dim=0)\n",
    "    return x_dataset, y_pred_dataset, scores_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0/10] Loss: 4.4973 Verifier_loss: 0.00 Acc: 17.00: 100%|██████████| 13/13 [00:01<00:00,  7.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed\n",
      "Train without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 21.00: 100%|██████████| 11/11 [00:07<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 17.00: 100%|██████████| 3/3 [00:02<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 15.00: 100%|██████████| 3/3 [00:02<00:00,  1.43it/s]\n"
     ]
    }
   ],
   "source": [
    "args.model = 'gpt2'\n",
    "args.epochs_sft = 10\n",
    "from transformers import GPT2LMHeadModel\n",
    "model = GPT2LMHeadModel.from_pretrained(args.model)\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=0.0)\n",
    "ctx = nullcontext() if device == 'cpu' else torch.amp.autocast(device_type=device, dtype=ptdtype)\n",
    "\n",
    "max_iters = len(train_loader) * args.epochs_sft\n",
    "lr_decay_iters = max_iters\n",
    "\n",
    "results = {}\n",
    "num_iters = 0\n",
    "\n",
    "for ep in range(args.epochs_sft):\n",
    "    train_bar = tqdm(train_loader)\n",
    "    total_loss, total_acc = AverageMeter(), AverageMeter()\n",
    "    verifier_loss = AverageMeter()\n",
    "    for x, y in train_bar:\n",
    "        with ctx:\n",
    "            # logits, loss, accs = model(x, y)\n",
    "            logits = model(x)['logits']\n",
    "            loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "            acc = torch.mean((torch.argmax(logits[:, -train_data.num_target_tokens, :], dim=-1) == y[:, -train_data.num_target_tokens]).float())\n",
    "\n",
    "        # verifier \n",
    "        if ep > 5:\n",
    "            num_prefix_tokens=train_data.num_prefix_tokens\n",
    "            num_target_tokens=train_data.num_target_tokens\n",
    "            y_tokens=train_data.num_target_tokens-train_data.pad_length\n",
    "            x = torch.cat([x, y[:, -1:]], dim=1)\n",
    "            y = x[:, -y_tokens:].clone()\n",
    "            x = x[:, :num_prefix_tokens].clone()\n",
    "            y_pred = []\n",
    "            scores = []\n",
    "            for i in range(args.n_samples):\n",
    "                with ctx:\n",
    "                    _y_pred = model.generate(x, min_length=num_prefix_tokens+num_target_tokens, \n",
    "                                max_length=num_prefix_tokens+num_target_tokens, temperature=1., top_p=0.99,\n",
    "                                do_sample=True, attention_mask = torch.ones_like(x), pad_token_id=2)\n",
    "                _scores = []\n",
    "                for i in range(y.shape[0]):\n",
    "                    _scores.append(int(tokenizer.decode(y[i]) in tokenizer.decode(_y_pred[i])))\n",
    "                y_pred.append(_y_pred)\n",
    "                scores.append(_scores)\n",
    "            scores = torch.tensor(scores).float().transpose(0, 1)\n",
    "            y_pred = torch.stack(y_pred, dim=0).transpose(0, 1)\n",
    "            y_pred = y_pred.reshape(-1, y_pred.shape[-1]) \n",
    "            sign = scores.reshape(-1) * 2 - 1.\n",
    "            # sign = scores.reshape(-1)\n",
    "            # sign = scores.reshape(-1) - 1. \n",
    "        \n",
    "            x = x.unsqueeze(0).repeat(args.n_samples, 1, 1).transpose(0, 1).reshape(-1, x.shape[-1])\n",
    "            x_new = torch.cat([x, y_pred], dim=1)\n",
    "            y_new = x_new.clone()\n",
    "            y_new[:, :x.shape[1]] = -1\n",
    "            x_new = x_new[:, :-1].clone()\n",
    "            y_new = y_new[:, 1:].clone()\n",
    "            sign = sign.cuda()\n",
    "            with ctx:\n",
    "                logits = model(x_new)['logits']\n",
    "                vloss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), y_new.view(-1), ignore_index=-1, reduction='none')\n",
    "                # print(vloss)\n",
    "                # vloss.sigmoid_()\n",
    "                vloss = torch.sigmoid(0.1 * vloss)\n",
    "                sign = sign.unsqueeze(1).repeat(1, x_new.shape[-1]).reshape(-1)\n",
    "                vloss = vloss * sign\n",
    "                vloss = vloss.mean()\n",
    "        else:\n",
    "            vloss = torch.tensor(0.)\n",
    "        verifier_loss.update(vloss.item(), x.shape[0] * train_data.num_target_tokens)\n",
    "        total_loss.update(loss.item(), x.shape[0] * train_data.num_target_tokens)\n",
    "        total_acc.update(acc.item(), x.shape[0] * train_data.num_target_tokens)\n",
    "        scaler.scale(loss + 0.1 * vloss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        num_iters += 1\n",
    "        train_bar.set_description(\n",
    "            'Epoch: [{}/{}] Loss: {:.4f} Verifier_loss: {:.2f} Acc: {:.2f}'.format(ep, args.epochs_sft, total_loss.get(), verifier_loss.get(),\n",
    "             total_acc.get(percentage=True))\n",
    "        )\n",
    "        verifier_loss = AverageMeter()\n",
    "        \n",
    "\n",
    "    # if ep > 1:\n",
    "    #     print(\"Generating samples\")\n",
    "    #     loader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=False)\n",
    "    #     loader.dataset.eval()\n",
    "    #     prompts, generations, scores = generate_and_score(\n",
    "    #         model, loader, tokenizer, ctx, temperature=0.5, top_p=0.9, n_samples=3, num_prefix_tokens=train_data.num_prefix_tokens, \n",
    "    #         num_target_tokens=train_data.num_target_tokens, y_tokens=train_data.num_target_tokens-train_data.pad_length)\n",
    "    #     loader.dataset.train() \n",
    "    #     flat_generations = generations.reshape(-1, generations.shape[-1]) \n",
    "    #     flat_scores = scores.reshape(-1) * 2 - 1.\n",
    "    #     flat_prompts = prompts.unsqueeze(0).repeat(3, 1, 1).transpose(0, 1).reshape(-1, prompts.shape[-1])\n",
    "    #     pref_dataset = torch.utils.data.TensorDataset(flat_prompts, flat_generations, flat_scores)\n",
    "        # print(\"Training verifier\")\n",
    "        # verifier_loss = AverageMeter()\n",
    "        # for _ in range(2):\n",
    "        #     loader_bar = tqdm(torch.utils.data.DataLoader(pref_dataset, batch_size=args.batch_size//2, shuffle=True))\n",
    "        #     for x, y, sign in loader_bar:\n",
    "        #         x = x.cuda()\n",
    "        #         y = y.cuda()\n",
    "        #         x_new = torch.cat([x, y], dim=1)\n",
    "        #         y_new = x_new.clone()\n",
    "        #         y_new[:, :x.shape[1]] = -1\n",
    "        #         x_new = x_new[:, :-1].clone()\n",
    "        #         y_new = y_new[:, 1:].clone()\n",
    "        #         sign = sign.cuda()\n",
    "        #         with ctx:\n",
    "        #             logits = model(x_new)['logits']\n",
    "        #             loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), y_new.view(-1), ignore_index=-1, reduction='none')\n",
    "        #             sign = sign.unsqueeze(1).repeat(1, x_new.shape[-1]).reshape(-1)\n",
    "        #             loss *= sign\n",
    "        #             loss = loss.mean()\n",
    "        #         verifier_loss.update(loss.item(), x.shape[0] * train_data.num_target_tokens)\n",
    "        #         scaler.scale(loss).backward()\n",
    "        #         scaler.step(optimizer)\n",
    "        #         scaler.update()\n",
    "        #         optimizer.zero_grad(set_to_none=True)\n",
    "        #         loader_bar.set_description(\n",
    "        #             'Verifier Loss: {:.4f}'.format(verifier_loss.get()),\n",
    "        #         )\n",
    "\n",
    "                \n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    print(f\"Epoch {ep} completed\")\n",
    "    print(\"Train without COT\")\n",
    "    loader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=False)\n",
    "    loader.dataset.eval()\n",
    "    evaluate(model, loader, tokenizer, ctx, num_prefix_tokens=train_data.num_prefix_tokens, y_tokens=train_data.num_target_tokens-train_data.pad_length, pad_length=train_data.pad_length)\n",
    "    loader.dataset.train()\n",
    "    print(\"Train with COT\")\n",
    "    loader_wcot = torch.utils.data.DataLoader(train_data_wcot, batch_size=args.batch_size, shuffle=False)\n",
    "    loader_wcot.dataset.eval()\n",
    "    evaluate(model, loader_wcot, tokenizer, ctx, num_prefix_tokens=train_data_wcot.num_prefix_tokens, y_tokens=train_data_wcot.num_target_tokens-train_data.pad_length, pad_length=0)\n",
    "    loader_wcot.dataset.train()\n",
    "    print(\"Test without COT\")\n",
    "    loader_eval = torch.utils.data.DataLoader(test_data, batch_size=args.batch_size, shuffle=False)\n",
    "    loader_eval.dataset.eval()\n",
    "    evaluate(model, loader_eval, tokenizer, ctx, num_prefix_tokens=test_data.num_prefix_tokens, y_tokens=test_data.num_target_tokens-test_data.pad_length, pad_length=test_data.pad_length)\n",
    "    loader_eval.dataset.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    x,\n",
    "    num_beams=10,\n",
    "    max_new_tokens=20,\n",
    "    num_return_sequences=2,\n",
    "    attention_mask=torch.ones_like(x),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['32,25|48,35|7,9|48,32|9,33|25,1|1,13|35,7/48,33=48,35,7,9,33<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       "       '32,25|48,35|7,9|48,32|9,33|25,1|1,13|35,7/48,33=48,35,7,9,33<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'],\n",
       "      dtype='<U212')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tokenizer.batch_decode(outputs)).reshape(x.shape[0], 2)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['40,36|31,46|31,40|5,42|46,47|47,0|36,5|0,48/31,42=31,40,36,5,42<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       "        '40,36|31,46|31,40|5,42|46,47|47,0|36,5|0,48/31,42=31,40,36,5,42<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<|endoftext|>'],\n",
       "       ['10,5|34,37|3,38|19,3|37,48|19,34|38,10|48,35/19,35=19,34,37,48,35<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n-\\n\\n\\n-\\n\\n\\n-\\n\\n-\\n\\n-\\n\\n\\n-',\n",
       "        '10,5|34,37|3,38|19,3|37,48|19,34|38,10|48,35/19,35=19,34,37,48,35<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'],\n",
       "       ['32,25|48,35|7,9|48,32|9,33|25,1|1,13|35,7/48,33=48,35,7,9,33<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       "        '32,25|48,35|7,9|48,32|9,33|25,1|1,13|35,7/48,33=48,35,7,9,33<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'],\n",
       "       ['39,20|13,36|28,33|42,13|36,28|22,11|20,22|42,39/42,33=42,13,36,28,33<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       "        '39,20|13,36|28,33|42,13|36,28|22,11|20,22|42,39/42,33=42,13,36,28,33<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'],\n",
       "       ['16,27|26,13|29,34|13,2|16,33|27,26|33,44|44,29/16,34=16,33,44,29,34<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       "        '16,27|26,13|29,34|13,2|16,33|27,26|33,44|44,29/16,34=16,33,44,29,34<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'],\n",
       "       ['18,34|34,40|22,41|38,45|41,38|32,48|48,18|32,22/32,45=32,22,41,38,45<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       "        '18,34|34,40|22,41|38,45|41,38|32,48|48,18|32,22/32,45=32,22,41,38,45<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'],\n",
       "       ['47,0|29,16|11,47|16,40|48,4|48,20|4,11|20,29/48,40=48,20,29,16,40<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       "        '47,0|29,16|11,47|16,40|48,4|48,20|4,11|20,29/48,40=48,20,29,16,40<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\"\\n\\n\\n\"\\n\\n\\n\\n\"\\n\\n\"\\n\\n\"\\n\"\\n'],\n",
       "       ['22,9|18,20|44,18|9,24|12,35|12,44|35,22|20,39/12,24=12,35,22,9,24<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n\\n\\n-\\n\\n\\n-\\n\\n-\\n\\n-\\n\\n-\\n\\n',\n",
       "        '22,9|18,20|44,18|9,24|12,35|12,44|35,22|20,39/12,24=12,35,22,9,24<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n\\n\\n-\\n\\n\\n-\\n\\n-\\n\\n\\n-\\n\\n\\n\\n'],\n",
       "       ['14,46|46,7|31,1|44,3|7,27|3,33|33,31|44,14/44,27=44,14,46,7,27<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       "        '14,46|46,7|31,1|44,3|7,27|3,33|33,31|44,14/44,27=44,14,46,7,27<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n-\\n\\n\\n\\n-\\n\\n-\\n\\n-\\n\\n\\n-\\n\\n'],\n",
       "       ['35,17|11,21|47,19|30,38|19,29|29,11|47,30|38,35/47,17=47,30,38,35,17<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n-\\n\\n-\\n\\n\\n-\\n\\n-\\n\\n\\n\\n-\\n-',\n",
       "        '35,17|11,21|47,19|30,38|19,29|29,11|47,30|38,35/47,17=47,30,38,35,17<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'],\n",
       "       ['27,11|13,15|11,0|0,43|39,12|15,39|12,49|13,27/13,49=49,12,39,15,13:13,15,39,12,12,49:13,49,12,39,12,49,12,49,',\n",
       "        '27,11|13,15|11,0|0,43|39,12|15,39|12,49|13,27/13,49=49,12,39,15,13:13,15,39,12,12,49:13,49,12,39,12,49,12,39,'],\n",
       "       ['40,4|4,9|8,40|6,8|21,42|48,41|6,48|41,21/6,9=6,8,40,4,9<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       "        '40,4|4,9|8,40|6,8|21,42|48,41|6,48|41,21/6,9=6,8,40,4,9<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'],\n",
       "       ['12,26|25,48|48,34|26,15|12,0|0,25|32,31|15,32/12,34=12,0,25,48,34<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       "        '12,26|25,48|48,34|26,15|12,0|0,25|32,31|15,32/12,34=12,0,25,48,34<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'],\n",
       "       ['36,16|9,15|15,11|11,48|33,36|9,49|48,29|49,33/9,16=16,36,33,49,9:9,49,33,36,9:9,49,29:9,49,33,9,15,11,',\n",
       "        '36,16|9,15|15,11|11,48|33,36|9,49|48,29|49,33/9,16=16,36,33,49,9:9,49,33,36,9:9,49,29,49,33,9,15,11,48,'],\n",
       "       ['31,4|14,27|1,34|14,31|11,1|4,48|48,0|27,11/14,34=14,27,11,1,34<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       "        '31,4|14,27|1,34|14,31|11,1|4,48|48,0|27,11/14,34=14,27,11,1,34<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'],\n",
       "       ['38,47|41,12|47,25|25,2|0,9|38,41|2,43|12,0/38,9=38,41,12,0,9<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n1\\n\\n2\\n3\\n\\n\\n4\\n\\n\\n5\\n\\n6\\n\\n',\n",
       "        '38,47|41,12|47,25|25,2|0,9|38,41|2,43|12,0/38,9=38,41,12,0,9<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n1\\n\\n2\\n3\\n\\n\\n4\\n\\n5\\n\\n6\\n\\n7'],\n",
       "       ['4,9|22,2|31,0|9,11|8,31|2,8|22,30|30,4/22,11=22,30,4,9,11<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       "        '4,9|22,2|31,0|9,11|8,31|2,8|22,30|30,4/22,11=22,30,4,9,11<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n<|endoftext|>'],\n",
       "       ['44,48|3,21|48,28|3,40|21,34|22,10|34,22|40,44/3,28=28,48,44,40,3:3,40,44,48,48,44,40,4,28,28,48,48,28,28,',\n",
       "        '44,48|3,21|48,28|3,40|21,34|22,10|34,22|40,44/3,28=28,48,44,40,3:3,40,44,48,48,44,40,4,28,28,48,44,40,34,'],\n",
       "       ['5,27|6,9|33,6|12,31|9,5|17,12|31,44|33,17/33,44=44,31,12,17,33:33,17,12,31,4,27,6,9,9,9,9,9,9,9,',\n",
       "        '5,27|6,9|33,6|12,31|9,5|17,12|31,44|33,17/33,44=44,31,12,17,33:33,17,12,31,4,27,9,5,27,9,5,27,12,31,'],\n",
       "       ['31,13|6,4|13,43|12,31|12,41|4,3|41,6|43,14/12,14=12,31,13,43,14<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       "        '31,13|6,4|13,43|12,31|12,41|4,3|41,6|43,14/12,14=12,31,13,43,14<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'],\n",
       "       ['9,32|10,30|2,13|19,2|38,9|10,40|30,19|40,38/10,32=10,40,38,9,32<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       "        '9,32|10,30|2,13|19,2|38,9|10,40|30,19|40,38/10,32=10,40,38,9,32<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n-\\n\\n\\n-\\n\\n-\\n-\\n\\n-\\n\\n-\\n\\n'],\n",
       "       ['23,31|24,23|41,43|43,20|24,45|32,29|31,32|45,41/24,20=20,43,41,45,24:24,45,41,43,43,43,43,43,43,43,43,43,43,43,',\n",
       "        '23,31|24,23|41,43|43,20|24,45|32,29|31,32|45,41/24,20=20,43,41,45,24:24,45,41,43,43,43,43,43,43,43,43,43,43,41,'],\n",
       "       ['36,30|30,11|42,4|43,12|46,42|4,43|29,36|46,29/46,11=46,29,36,30,11<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       "        '36,30|30,11|42,4|43,12|46,42|4,43|29,36|46,29/46,11=46,29,36,30,11<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|> \"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'],\n",
       "       ['2,27|27,42|29,9|0,31|29,3|3,2|9,18|18,0/29,31=29,9,18,0,31<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n-\\n-\\n\\n-\\n\\n\\n-\\n\\n\\n-\\n\\n\\n-\\n',\n",
       "        '2,27|27,42|29,9|0,31|29,3|3,2|9,18|18,0/29,31=29,9,18,0,31<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\\n-\\n\\n-\\n\\n\\n-\\n\\n-\\n\\n-\\n\\n\\n-\\n']],\n",
       "      dtype='<U212')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tokenizer.batch_decode(outputs)).reshape(x.shape[0], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['23,4|6,19|24,12|24,23|7,39|4,6|12,25|25,7/24,39=24,12,25,7,39<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '21,19|36,5|33,22|25,33|3,36|19,3|22,34|21,25/21,5=21,19,3,36,5<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '12,22|37,49|49,12|20,14|22,16|14,5|37,20|5,23/37,16=37,49,12,22,16<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '18,24|39,34|24,4|4,28|33,29|18,39|28,41|34,33/18,41=18,24,4,28,41<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '8,21|41,14|36,1|36,27|27,0|0,41|1,8|21,42/36,14=36,27,0,41,14<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '35,37|4,36|28,35|37,4|22,32|45,41|28,45|41,22/28,32=28,45,41,22,32<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '45,49|49,0|39,30|45,39|0,42|30,16|42,44|16,22/45,22=45,39,30,16,22<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '16,44|31,48|44,25|14,16|22,14|19,12|22,19|12,31/22,48=22,19,12,31,48<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '22,48|48,13|2,15|49,21|29,22|21,11|15,49|2,29/2,11=2,15,49,21,11<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '5,1|1,6|31,21|43,19|6,43|5,11|48,31|11,48/5,21=5,11,48,31,21<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '23,44|20,27|16,23|44,5|27,38|10,20|35,10|35,16/35,38=35,10,20,27,38<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '26,9|23,26|18,46|25,13|46,25|3,23|4,3|4,18/4,13=4,18,46,25,13<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '21,45|38,27|27,4|4,35|45,5|9,38|9,21|5,1/9,35=9,38,27,4,35<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '19,1|45,24|22,29|43,22|1,17|45,47|47,43|24,19/45,29=45,47,43,22,29<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '29,1|23,22|1,24|22,46|30,35|23,32|32,29|46,30/23,35=23,22,46,30,35<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '30,10|32,14|32,31|31,30|9,47|47,3|14,9|10,0/32,3=32,14,9,47,3<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '25,29|29,16|3,21|0,49|21,12|0,31|49,3|31,25/0,12=0,49,3,21,12<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '0,33|49,5|39,49|41,47|47,0|4,39|36,4|36,41/36,5=36,4,39,49,5<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '17,12|22,28|49,17|9,24|49,9|13,7|12,22|24,13/49,7=49,9,24,13,7<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '32,33|33,6|6,0|0,23|46,15|17,28|15,17|32,46/32,23=32,33,6,0,23<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '31,6|48,19|6,29|34,11|11,45|48,31|29,18|19,34/48,18=48,31,6,29,18<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '12,32|47,12|25,2|15,42|47,14|32,15|46,25|14,46/47,42=47,12,32,15,42<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '39,40|49,41|38,17|48,49|21,48|17,39|40,44|38,21/38,44=38,17,39,40,44<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '47,23|28,36|4,47|29,22|31,4|22,28|31,29|23,37/31,36=31,29,22,28,36<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '34,22|26,39|46,9|25,26|39,34|25,8|15,46|8,15/25,22=25,26,39,34,22<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '39,3|18,49|29,15|0,18|17,34|0,17|49,39|34,29/0,15=0,17,34,29,15<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '34,48|22,40|30,23|48,49|49,1|40,30|23,45|22,34/22,1=22,34,48,49,1<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '46,1|1,5|5,39|8,30|49,46|28,8|49,22|22,28/49,39=49,46,1,5,39<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '32,3|16,12|3,19|32,34|34,6|6,16|19,47|47,28/32,12=32,34,6,16,12<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '35,3|3,22|22,12|12,31|43,20|49,5|5,43|35,49/35,20=35,49,5,43,20<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '3,7|26,3|7,10|12,38|10,45|26,35|38,47|35,12/26,47=26,35,12,38,47<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '1,47|0,20|19,1|47,17|34,0|20,40|34,19|40,10/34,17=34,19,1,47,17<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '7,35|31,34|34,7|29,31|43,39|40,30|30,43|29,40/29,39=29,40,30,43,39<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '0,23|49,18|20,1|8,7|39,0|23,8|39,49|18,20/39,1=39,49,18,20,1<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '44,8|8,4|4,42|20,44|37,7|38,49|20,37|7,38/20,49=20,37,7,38,49<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '21,47|44,9|9,21|43,15|38,44|35,43|48,35|38,48/38,15=38,48,35,43,15<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '33,13|0,17|0,26|17,43|24,44|26,45|43,33|45,24/0,13=0,17,43,33,13<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '30,10|29,32|24,12|35,29|24,46|12,19|46,35|19,30/24,32=24,46,35,29,32<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '5,8|32,5|21,2|42,32|2,40|42,21|8,4|40,18/42,18=42,21,2,40,18<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '13,36|30,11|7,12|7,13|12,30|16,34|11,1|36,16/7,1=7,12,30,11,1<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '10,26|15,19|30,10|45,15|20,27|45,30|26,22|19,20/45,27=45,15,19,20,27<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '0,29|42,7|7,11|11,4|41,21|4,45|21,0|42,41/42,45=42,7,11,4,45<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '12,26|32,34|46,12|48,30|26,48|42,32|46,31|31,42/46,30=46,12,26,48,30<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '17,39|1,46|46,31|39,18|8,43|43,1|8,25|25,17/8,18=8,25,17,39,18<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '37,19|46,24|19,25|21,8|25,2|46,37|24,5|5,21/46,2=46,37,19,25,2<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '14,9|24,10|47,32|46,47|46,25|32,14|25,24|10,34/46,34=46,25,24,10,34<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '25,6|6,5|25,36|36,8|23,13|5,48|8,23|48,46/25,46=25,6,5,48,46<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       " '31,12|12,22|48,46|30,32|3,31|32,26|46,30|48,3/48,26=48,46,30,32,26<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(next(test_loader.__iter__())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syth-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
