{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asetlur/anaconda3/envs/syth-llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-18 04:15:55,698] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data import get_dataset\n",
    "from utils.training_utils import get_lr, get_run_name, AverageMeter\n",
    "from torch.utils.data import DataLoader\n",
    "from models import get_model\n",
    "from tokenizing import get_tokenizer\n",
    "import wandb\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from copy import deepcopy\n",
    "from transformers import TrainingArguments\n",
    "from trl import DPOTrainer\n",
    "from transformers import AutoTokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Next-token failures\")\n",
    "# Data\n",
    "parser.add_argument(\n",
    "    \"--n_samples\", type=int, default=5, help=\"Number of samples to generate\"\n",
    "    )\n",
    "parser.add_argument(\n",
    "    \"--model\", default='gpt2', type=str, help=\"Type of model\"\n",
    "    )\n",
    "parser.add_argument(\n",
    "    \"--dataset\", default='graph', type=str, help=\"Choice of dataset\"\n",
    "    )\n",
    "parser.add_argument(\n",
    "    \"--n_train\", default=200000, type=int, help=\"Number of training samples\"\n",
    "    )\n",
    "parser.add_argument(\n",
    "    \"--n_test\", default=500, type=int, help=\"Number of test samples\"\n",
    "    )\n",
    "parser.add_argument(\n",
    "    \"--num_nodes\", default=50, type=int, help=\"Number of node values in graph\"\n",
    "    )\n",
    "parser.add_argument(\n",
    "    \"--deg\", default=2, type=int, help=\"Degree of starting node\"\n",
    "    )\n",
    "parser.add_argument(\n",
    "    \"--path_len\", default=5, type=int, help=\"Path length in star graph\"\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--mate_in\", default=2, type=int, help=\"For chess, number of moves to checkmate\"\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--unrolled\", action=argparse.BooleanOptionalAction, default=True, help=\"For chess, unrolled board state\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--batch_size\", type=int, default=64, help=\"Batch size\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--lr\", type=float, default=5e-4, help=\"Learning rate\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--weight_decay\", type=float, default=0., help=\"Strength of weight decay\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--epochs_sft\", type=int, default=1, help=\"Number of SFT epochs\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--save_every\", type=int, default=5000, help=\"Interval (in steps) at which to save model\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--pass_at_k\", type=int, default=1, help=\"pass at k eval\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--teacherless\", action=argparse.BooleanOptionalAction, default=False, help=\"Standard or teacherless training\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--reverse\", action=argparse.BooleanOptionalAction, default=False, help=\"Standard format or reverse targets\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--cot\", action=argparse.BooleanOptionalAction, default=False, help=\"Standard format or cot targets\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--pos\", action=argparse.BooleanOptionalAction, default=False, help=\"Standard format or pos tokens\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--eval_train\", action=argparse.BooleanOptionalAction, default=False, help=\"Eval for training set\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--eval_every\", type=int, default=400, help=\"Interval (in steps) to evaluate the model on test\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--use_wandb\", action=argparse.BooleanOptionalAction, default=False, help=\"Whether to use wandb\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--wandb_entity\", type=str, default=5000, help=\"Wandb username\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "    \"--pad_length\", default=0, type=int, help=\"Default value for pad length\"\n",
    ")\n",
    "\n",
    "\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System stuff\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "wandb_entity = args.wandb_entity\n",
    "wandb_log = args.use_wandb\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "\n",
    "# Model stuff\n",
    "top_k = 1000\n",
    "temperature = 1.\n",
    "pass_at_k = args.pass_at_k\n",
    "n_samples = args.n_samples\n",
    "\n",
    "# Evaluation stuff\n",
    "eval_iters = 1000\n",
    "eval_interval = 5\n",
    "log_interval = 10\n",
    "\n",
    "# Optimiser\n",
    "dtype = 'bfloat16'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "decay_lr = True\n",
    "args.compile = False if device == 'cuda' else False\n",
    "args.use_flash = True if device == 'cuda' else False\n",
    "warmup_iters = 100\n",
    "min_lr = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal sequence lengths!\n",
      "Equal sequence lengths!\n",
      "Equal sequence lengths!\n",
      "Equal sequence lengths!\n",
      "Equal sequence lengths!\n",
      "1000 torch.Size([44]) 100 torch.Size([54])\n",
      "W/o COT (tensor([2920,   11, 1485,   91, 1954,   11, 2078,   91, 3510,   11, 2327,   91,\n",
      "        2624,   11, 3682,   91, 2327,   11, 2624,   91, 3682,   11, 1828,   91,\n",
      "        1485,   11, 1954,   91, 3510,   11, 2920,   14, 3510,   11, 1828,   28,\n",
      "        3510,   11, 2327,   11, 2624,   11, 3682,   11], device='cuda:0'), tensor([  -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "          -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "          -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1, 3510,\n",
      "          11, 2327,   11, 2624,   11, 3682,   11, 1828], device='cuda:0')) 49,13|23,28|46,35|32,42|35,32|42,22|13,23|46,49/46,22=46,35,32,42,\n",
      "With COT (tensor([3559,   11, 2682,   91, 1238,   11,   24,   91, 2780,   11,   23,   91,\n",
      "          24,   11, 2075,   91, 1983,   11, 1238,   91, 3559,   11, 1983,   91,\n",
      "          23,   11, 2598,   91, 2682,   11, 2780,   14, 3559,   11, 2598,   28,\n",
      "        2598,   11,   23,   11, 2780,   11, 2682,   11, 3559,   25, 3559,   11,\n",
      "        2682,   11, 2780,   11,   23,   11], device='cuda:0'), tensor([  -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "          -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "          -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1, 2598,\n",
      "          11,   23,   11, 2780,   11, 2682,   11, 3559,   25, 3559,   11, 2682,\n",
      "          11, 2780,   11,   23,   11, 2598], device='cuda:0')) 43,34|20,9|48,8|9,26|27,20|43,27|8,44|34,48/43,44=44,8,48,34,43:43,34,48,8,\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(args)\n",
    "\n",
    "# train data without cot\n",
    "args.cot = False\n",
    "args.n_train = 1000\n",
    "args.n_test = 100\n",
    "args.pad_length = 0\n",
    "train_data, test_data = get_dataset(args, tokenizer, device)\n",
    "train_data.num_target_tokens += train_data.pad_length\n",
    "test_data.num_target_tokens += test_data.pad_length\n",
    "train_data.num_tokens += train_data.pad_length\n",
    "test_data.num_tokens += test_data.pad_length\n",
    "\n",
    "# train data with cot\n",
    "args.cot = True\n",
    "args.n_train = 100\n",
    "args.pad_length = 0\n",
    "train_data_wcot, _ = get_dataset(args, tokenizer, device)\n",
    "train_data_wcot.data_file = train_data_wcot.data_file[-args.n_train:]\n",
    "train_data_wcot.tokenized, train_data_wcot.num_prefix_tokens, train_data_wcot.num_target_tokens = train_data_wcot.tokenizer.tokenize(train_data_wcot.data_file)\n",
    "\n",
    "\n",
    "print(len(train_data), train_data[0][0].shape, len(train_data_wcot), train_data_wcot[0][0].shape)\n",
    "print(\"W/o COT\", train_data[0], tokenizer.decode(train_data[0][0]))\n",
    "print(\"With COT\", train_data_wcot[0], tokenizer.decode(train_data_wcot[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 9, 45, 36, 19, 55, 36, 9, 45)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.num_prefix_tokens, train_data.num_target_tokens, train_data.num_tokens, train_data_wcot.num_prefix_tokens, train_data_wcot.num_target_tokens, train_data_wcot.num_tokens, test_data.num_prefix_tokens, test_data.num_target_tokens, test_data.num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from torch.utils.data import ConcatDataset\n",
    "# concatenated_train_data = ConcatDataset([train_data, train_data_wcot])\n",
    "# train_loader = DataLoader(concatenated_train_data, batch_size=args.batch_size, shuffle=True)\n",
    "train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=args.batch_size, shuffle=True)\n",
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, tokenizer, ctx, num_prefix_tokens, y_tokens, pad_length):\n",
    "    model.eval()\n",
    "    total_acc = AverageMeter()\n",
    "    bar = tqdm(loader)\n",
    "    for x in bar:\n",
    "        original_length = x.shape[1]\n",
    "        y = x[:, -y_tokens:].clone()\n",
    "        x = x[:, :num_prefix_tokens].clone()\n",
    "        with ctx:\n",
    "            y_pred = model.generate(x, max_new_tokens=original_length + y_tokens, min_length=original_length + y_tokens,\n",
    "                                do_sample=False, attention_mask = torch.ones_like(x), pad_token_id=2)\n",
    "        completely_correct = 0 \n",
    "        for i in range(y.shape[0]):\n",
    "            completely_correct += int(tokenizer.decode(y[i]) in tokenizer.decode(y_pred[i]))\n",
    "        completely_correct /= x.shape[0]\n",
    "        total_acc.update(completely_correct, x.shape[0])\n",
    "        bar.set_description(f' accuracy: {total_acc.get(percentage=True):.2f}')\n",
    "    loader.dataset.train()\n",
    "    model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_score(model, loader, tokenizer, ctx, temperature, top_p, n_samples, num_prefix_tokens, num_target_tokens, y_tokens):\n",
    "    model.eval()\n",
    "    bar = tqdm(loader)\n",
    "    x_dataset = []\n",
    "    y_pred_dataset = []\n",
    "    scores_dataset = []\n",
    "    for x in bar:\n",
    "        original_length = x.shape[1]\n",
    "        y = x[:, -y_tokens:].clone()\n",
    "        x = x[:, :num_prefix_tokens].clone()\n",
    "        y = y\n",
    "        y_pred = []\n",
    "        scores = []\n",
    "        for i in range(n_samples):\n",
    "            with ctx:\n",
    "                # print(x.shape, num_target_tokens)\n",
    "                _y_pred = model.generate(x, min_length=num_prefix_tokens+num_target_tokens, \n",
    "                                   max_length=num_prefix_tokens+num_target_tokens, temperature=temperature, top_p=top_p,\n",
    "                                   do_sample=True, attention_mask = torch.ones_like(x), pad_token_id=2)\n",
    "            _scores = []\n",
    "            for i in range(y.shape[0]):\n",
    "                _scores.append(int(tokenizer.decode(y[i]) in tokenizer.decode(_y_pred[i])))\n",
    "            # print(_y_pred.shape)\n",
    "            y_pred.append(_y_pred.cpu())\n",
    "            scores.append(_scores)\n",
    "            \n",
    "        scores = torch.tensor(scores).float().transpose(0, 1)\n",
    "        y_pred = torch.stack(y_pred, dim=0).transpose(0, 1)\n",
    "        # y_pred should be bs x nsamples x length\n",
    "        y_pred_dataset.append(y_pred[:, :, num_prefix_tokens:].cpu())\n",
    "        scores_dataset.append(scores.cpu())\n",
    "        # print(scores.shape, y_pred.shape, x.shape)\n",
    "        x_dataset.append(x.cpu())\n",
    "        # correct should be bs x nsamples\n",
    "    # Switch back to train mode\n",
    "    loader.dataset.train()\n",
    "    model.train()\n",
    "    y_pred_dataset = torch.cat(y_pred_dataset, dim=0)\n",
    "    x_dataset = torch.cat(x_dataset, dim=0)\n",
    "    scores_dataset = torch.cat(scores_dataset, dim=0)\n",
    "    return x_dataset, y_pred_dataset, scores_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0/15] Loss: 1.0472 Verifier_loss: 0.00 Scores: 0.00 Acc: 29.80: 100%|██████████| 32/32 [00:08<00:00,  3.61it/s]\n",
      " accuracy: 54.00: 100%|██████████| 2/2 [00:03<00:00,  1.73s/it]\n",
      "Epoch: [1/15] Loss: 0.1814 Verifier_loss: 0.00 Scores: 0.00 Acc: 98.40: 100%|██████████| 32/32 [00:08<00:00,  3.61it/s]\n",
      " accuracy: 53.00: 100%|██████████| 2/2 [00:03<00:00,  1.74s/it]\n",
      "Epoch: [2/15] Loss: 0.1210 Verifier_loss: 0.00 Scores: 0.00 Acc: 100.00: 100%|██████████| 32/32 [00:08<00:00,  3.62it/s]\n",
      " accuracy: 64.00: 100%|██████████| 2/2 [00:03<00:00,  1.79s/it]\n",
      "Epoch: [3/15] Loss: 0.0935 Verifier_loss: 0.00 Scores: 0.00 Acc: 100.00: 100%|██████████| 32/32 [00:08<00:00,  3.62it/s]\n",
      " accuracy: 68.00: 100%|██████████| 2/2 [00:03<00:00,  1.74s/it]\n",
      "Epoch: [4/15] Loss: 0.0781 Verifier_loss: 0.00 Scores: 0.00 Acc: 100.00: 100%|██████████| 32/32 [00:08<00:00,  3.61it/s]\n",
      " accuracy: 59.00: 100%|██████████| 2/2 [00:03<00:00,  1.80s/it]\n",
      "Epoch: [5/15] Loss: 0.0593 Verifier_loss: 0.00 Scores: 0.00 Acc: 100.00: 100%|██████████| 32/32 [00:08<00:00,  3.60it/s]\n",
      " accuracy: 58.00: 100%|██████████| 2/2 [00:03<00:00,  1.82s/it]\n",
      "Epoch: [6/15] Loss: 0.0449 Verifier_loss: 0.00 Scores: 0.00 Acc: 100.00: 100%|██████████| 32/32 [00:08<00:00,  3.60it/s]\n",
      " accuracy: 60.00: 100%|██████████| 2/2 [00:03<00:00,  1.76s/it]\n",
      "Epoch: [7/15] Loss: 0.0390 Verifier_loss: 0.00 Scores: 0.00 Acc: 100.00: 100%|██████████| 32/32 [00:08<00:00,  3.60it/s]\n",
      " accuracy: 62.00: 100%|██████████| 2/2 [00:03<00:00,  1.82s/it]\n",
      "Epoch: [8/15] Loss: 0.0337 Verifier_loss: 0.00 Scores: 0.00 Acc: 100.00: 100%|██████████| 32/32 [00:08<00:00,  3.59it/s]\n",
      " accuracy: 58.00: 100%|██████████| 2/2 [00:03<00:00,  1.74s/it]\n",
      "Epoch: [9/15] Loss: 0.0258 Verifier_loss: 0.00 Scores: 0.00 Acc: 100.00: 100%|██████████| 32/32 [00:08<00:00,  3.59it/s]\n",
      " accuracy: 56.00: 100%|██████████| 2/2 [00:03<00:00,  1.83s/it]\n",
      "Epoch: [10/15] Loss: 0.0159 Verifier_loss: 0.00 Scores: 0.00 Acc: 100.00: 100%|██████████| 32/32 [00:08<00:00,  3.59it/s]\n",
      " accuracy: 57.00: 100%|██████████| 2/2 [00:03<00:00,  1.80s/it]\n",
      "Epoch: [11/15] Loss: 0.0108 Verifier_loss: 0.00 Scores: 0.00 Acc: 100.00: 100%|██████████| 32/32 [00:08<00:00,  3.59it/s]\n",
      " accuracy: 56.00: 100%|██████████| 2/2 [00:03<00:00,  1.80s/it]\n",
      "Epoch: [12/15] Loss: 0.0443 Verifier_loss: 0.00 Scores: 0.00 Acc: 99.90: 100%|██████████| 32/32 [00:08<00:00,  3.58it/s] \n",
      " accuracy: 62.00: 100%|██████████| 2/2 [00:03<00:00,  1.76s/it]\n",
      "Epoch: [13/15] Loss: 0.0241 Verifier_loss: 0.00 Scores: 0.00 Acc: 100.00: 100%|██████████| 32/32 [00:08<00:00,  3.57it/s]\n",
      " accuracy: 56.00: 100%|██████████| 2/2 [00:03<00:00,  1.76s/it]\n",
      "Epoch: [14/15] Loss: 0.0166 Verifier_loss: 0.00 Scores: 0.00 Acc: 100.00: 100%|██████████| 32/32 [00:08<00:00,  3.58it/s]\n",
      " accuracy: 60.00: 100%|██████████| 2/2 [00:03<00:00,  1.82s/it]\n"
     ]
    }
   ],
   "source": [
    "args.lr = 1e-5\n",
    "args.model = 'gpt2-large'\n",
    "args.epochs_sft = 15\n",
    "from transformers import GPT2LMHeadModel\n",
    "model = GPT2LMHeadModel.from_pretrained(args.model)\n",
    "model.to(device)\n",
    "model.train()\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=0.0)\n",
    "ctx = nullcontext() if device == 'cpu' else torch.amp.autocast(device_type=device, dtype=ptdtype)\n",
    "\n",
    "max_iters = len(train_loader) * args.epochs_sft\n",
    "lr_decay_iters = max_iters\n",
    "\n",
    "results = {}\n",
    "num_iters = 0\n",
    "\n",
    "for ep in range(args.epochs_sft):\n",
    "    train_bar = tqdm(train_loader)\n",
    "    total_loss, total_acc = AverageMeter(), AverageMeter()\n",
    "    verifier_loss = AverageMeter()\n",
    "    scores_meter = AverageMeter()\n",
    "    for x, y in train_bar:\n",
    "        # determine and set the learning rate for this iteration\n",
    "        lr = get_lr(num_iters, args.lr, warmup_iters, lr_decay_iters, min_lr) if decay_lr else args.lr\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        with ctx:\n",
    "            # logits, loss, accs = model(x, y)\n",
    "            logits = model(x)['logits']\n",
    "            loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "            acc = torch.mean((torch.argmax(logits[:, -train_data.num_target_tokens, :], dim=-1) == y[:, -train_data.num_target_tokens]).float())\n",
    "\n",
    "        if ep >= 100:\n",
    "            outputs = model.generate(\n",
    "                x[:, :train_data.num_prefix_tokens],\n",
    "                num_beams=2,\n",
    "                max_new_tokens=9,\n",
    "                num_return_sequences=2,\n",
    "                temperature=1.0,\n",
    "                attention_mask=torch.ones_like(x[:, :train_data.num_prefix_tokens]),\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            y_tokens=train_data.num_target_tokens-train_data.pad_length\n",
    "            scores = []\n",
    "            for i in range(outputs.shape[0]):\n",
    "                scores.append(int(tokenizer.decode(outputs[i, -y_tokens:]) in tokenizer.decode(y[i // 2, -y_tokens:])))\n",
    "            scores = torch.tensor(scores).float().cuda()\n",
    "            x_new = outputs[:, :-1].clone()\n",
    "            y_new = outputs.clone()\n",
    "            y_new[:, :train_data.num_prefix_tokens] = -1\n",
    "            y_new = y_new[:, 1:]\n",
    "            logits = model(x_new)['logits']\n",
    "            # print(logits.shape, y_new.shape)\n",
    "            vloss = torch.nn.functional.cross_entropy(logits.reshape(-1, logits.size(-1)), y_new.reshape(-1), ignore_index=-1, reduction='none')\n",
    "            sign = (2 * scores - 1.)\n",
    "            sign = sign.unsqueeze(1).repeat(1, x_new.shape[-1])\n",
    "            vloss = vloss.reshape(x_new.shape[0], x_new.shape[1])\n",
    "            # print(vloss.shape, sign.shape)\n",
    "            vloss = (vloss * sign).sum(1).mean(0)\n",
    "            vloss = -torch.log(torch.sigmoid(vloss))\n",
    "            vloss = vloss.mean()\n",
    "        else:\n",
    "            vloss = torch.tensor(0.)\n",
    "            scores = torch.tensor(0.)\n",
    "        verifier_loss.update(vloss.item(), x.shape[0] * train_data.num_target_tokens)\n",
    "        scores_meter.update(scores.mean().item(), x.shape[0]*2)\n",
    "        total_loss.update(loss.item(), x.shape[0] * train_data.num_target_tokens)\n",
    "        total_acc.update(acc.item(), x.shape[0] * train_data.num_target_tokens)\n",
    "\n",
    "        scaler.scale(loss + vloss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        num_iters += 1\n",
    "        \n",
    "        train_bar.set_description(\n",
    "            'Epoch: [{}/{}] Loss: {:.4f} Verifier_loss: {:.2f} Scores: {:.2f} Acc: {:.2f}'.format(ep, args.epochs_sft, total_loss.get(), verifier_loss.get(), scores_meter.get(),\n",
    "             total_acc.get(percentage=True))\n",
    "        )\n",
    "        # if num_iters % 50 == 0:\n",
    "        #     loader_eval = torch.utils.data.DataLoader(test_data, batch_size=args.batch_size, shuffle=False)\n",
    "        #     loader_eval.dataset.eval()\n",
    "        #     evaluate(model, loader_eval, tokenizer, ctx, num_prefix_tokens=test_data.num_prefix_tokens, y_tokens=test_data.num_target_tokens-test_data.pad_length, pad_length=test_data.pad_length)\n",
    "        #     loader_eval.dataset.train()\n",
    "\n",
    "\n",
    "        \n",
    "                \n",
    "            # x = torch.cat([x, y[:, -1:]], dim=1)\n",
    "            # y = x[:, -y_tokens:].clone()\n",
    "            # x = x[:, :num_prefix_tokens].clone()\n",
    "            # y_pred = []\n",
    "            # scores = []\n",
    "                        \n",
    "        # verifier \n",
    "        # if ep > 5:\n",
    "        #     num_prefix_tokens=train_data.num_prefix_tokens\n",
    "        #     num_target_tokens=train_data.num_target_tokens\n",
    "        #     y_tokens=train_data.num_target_tokens-train_data.pad_length\n",
    "        #     x = torch.cat([x, y[:, -1:]], dim=1)\n",
    "        #     y = x[:, -y_tokens:].clone()\n",
    "        #     x = x[:, :num_prefix_tokens].clone()\n",
    "        #     y_pred = []\n",
    "        #     scores = []\n",
    "        #     for i in range(args.n_samples):\n",
    "        #         with ctx:\n",
    "        #             _y_pred = model.generate(x, min_length=num_prefix_tokens+num_target_tokens, \n",
    "        #                         max_length=num_prefix_tokens+num_target_tokens, temperature=1., top_p=0.99,\n",
    "        #                         do_sample=True, attention_mask = torch.ones_like(x), pad_token_id=2)\n",
    "        #         _scores = []\n",
    "        #         for i in range(y.shape[0]):\n",
    "        #             _scores.append(int(tokenizer.decode(y[i]) in tokenizer.decode(_y_pred[i])))\n",
    "        #         y_pred.append(_y_pred)\n",
    "        #         scores.append(_scores)\n",
    "        #     scores = torch.tensor(scores).float().transpose(0, 1)\n",
    "        #     y_pred = torch.stack(y_pred, dim=0).transpose(0, 1)\n",
    "        #     y_pred = y_pred.reshape(-1, y_pred.shape[-1]) \n",
    "        #     sign = scores.reshape(-1) * 2 - 1.\n",
    "        #     # sign = scores.reshape(-1)\n",
    "        #     # sign = scores.reshape(-1) - 1. \n",
    "        \n",
    "        #     x = x.unsqueeze(0).repeat(args.n_samples, 1, 1).transpose(0, 1).reshape(-1, x.shape[-1])\n",
    "        #     x_new = torch.cat([x, y_pred], dim=1)\n",
    "        #     y_new = x_new.clone()\n",
    "        #     y_new[:, :x.shape[1]] = -1\n",
    "        #     x_new = x_new[:, :-1].clone()\n",
    "        #     y_new = y_new[:, 1:].clone()\n",
    "        #     sign = sign.cuda()\n",
    "        #     with ctx:\n",
    "        #         logits = model(x_new)['logits']\n",
    "        #         vloss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), y_new.view(-1), ignore_index=-1, reduction='none')\n",
    "        #         # print(vloss)\n",
    "        #         # vloss.sigmoid_()\n",
    "        #         vloss = torch.sigmoid(0.1 * vloss)\n",
    "        #         sign = sign.unsqueeze(1).repeat(1, x_new.shape[-1]).reshape(-1)\n",
    "        #         vloss = vloss * sign\n",
    "        #         vloss = vloss.mean()\n",
    "        # else:\n",
    "        #   vloss = torch.tensor(0.)\n",
    "        # verifier_loss.update(vloss.item(), x.shape[0] * train_data.num_target_tokens)\n",
    "\n",
    "\n",
    "        # verifier_loss = AverageMeter()\n",
    "        \n",
    "\n",
    "    # if ep > 1:\n",
    "    #     print(\"Generating samples\")\n",
    "    #     loader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=False)\n",
    "    #     loader.dataset.eval()\n",
    "    #     prompts, generations, scores = generate_and_score(\n",
    "    #         model, loader, tokenizer, ctx, temperature=0.5, top_p=0.9, n_samples=3, num_prefix_tokens=train_data.num_prefix_tokens, \n",
    "    #         num_target_tokens=train_data.num_target_tokens, y_tokens=train_data.num_target_tokens-train_data.pad_length)\n",
    "    #     loader.dataset.train() \n",
    "    #     flat_generations = generations.reshape(-1, generations.shape[-1]) \n",
    "    #     flat_scores = scores.reshape(-1) * 2 - 1.\n",
    "    #     flat_prompts = prompts.unsqueeze(0).repeat(3, 1, 1).transpose(0, 1).reshape(-1, prompts.shape[-1])\n",
    "    #     pref_dataset = torch.utils.data.TensorDataset(flat_prompts, flat_generations, flat_scores)\n",
    "        # print(\"Training verifier\")\n",
    "        # verifier_loss = AverageMeter()\n",
    "        # for _ in range(2):\n",
    "        #     loader_bar = tqdm(torch.utils.data.DataLoader(pref_dataset, batch_size=args.batch_size//2, shuffle=True))\n",
    "        #     for x, y, sign in loader_bar:\n",
    "        #         x = x.cuda()\n",
    "        #         y = y.cuda()\n",
    "        #         x_new = torch.cat([x, y], dim=1)\n",
    "        #         y_new = x_new.clone()\n",
    "        #         y_new[:, :x.shape[1]] = -1\n",
    "        #         x_new = x_new[:, :-1].clone()\n",
    "        #         y_new = y_new[:, 1:].clone()\n",
    "        #         sign = sign.cuda()\n",
    "        #         with ctx:\n",
    "        #             logits = model(x_new)['logits']\n",
    "        #             loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), y_new.view(-1), ignore_index=-1, reduction='none')\n",
    "        #             sign = sign.unsqueeze(1).repeat(1, x_new.shape[-1]).reshape(-1)\n",
    "        #             loss *= sign\n",
    "        #             loss = loss.mean()\n",
    "        #         verifier_loss.update(loss.item(), x.shape[0] * train_data.num_target_tokens)\n",
    "        #         scaler.scale(loss).backward()\n",
    "        #         scaler.step(optimizer)\n",
    "        #         scaler.update()\n",
    "        #         optimizer.zero_grad(set_to_none=True)\n",
    "        #         loader_bar.set_description(\n",
    "        #             'Verifier Loss: {:.4f}'.format(verifier_loss.get()),\n",
    "        #         )\n",
    "\n",
    "                \n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    # print(f\"Epoch {ep} completed\")\n",
    "    # print(\"Train without COT\")\n",
    "    # loader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=False)\n",
    "    # loader.dataset.eval()\n",
    "    # evaluate(model, loader, tokenizer, ctx, num_prefix_tokens=train_data.num_prefix_tokens, y_tokens=train_data.num_target_tokens-train_data.pad_length, pad_length=train_data.pad_length)\n",
    "    # loader.dataset.train()\n",
    "    # print(\"Train with COT\")\n",
    "    # loader_wcot = torch.utils.data.DataLoader(train_data_wcot, batch_size=args.batch_size, shuffle=False)\n",
    "    # loader_wcot.dataset.eval()\n",
    "    # evaluate(model, loader_wcot, tokenizer, ctx, num_prefix_tokens=train_data_wcot.num_prefix_tokens, y_tokens=train_data_wcot.num_target_tokens-train_data.pad_length, pad_length=0)\n",
    "    # loader_wcot.dataset.train()\n",
    "    # print(\"Test without COT\")\n",
    "    loader_eval = torch.utils.data.DataLoader(test_data, batch_size=args.batch_size, shuffle=False)\n",
    "    loader_eval.dataset.eval()\n",
    "    evaluate(model, loader_eval, tokenizer, ctx, num_prefix_tokens=test_data.num_prefix_tokens, y_tokens=test_data.num_target_tokens-test_data.pad_length, pad_length=test_data.pad_length)\n",
    "    loader_eval.dataset.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 56.00: 100%|██████████| 2/2 [00:03<00:00,  1.65s/it]\n"
     ]
    }
   ],
   "source": [
    "loader_eval = torch.utils.data.DataLoader(test_data, batch_size=args.batch_size, shuffle=False)\n",
    "loader_eval.dataset.eval()\n",
    "evaluate(model, loader_eval, tokenizer, ctx, num_prefix_tokens=test_data.num_prefix_tokens, y_tokens=test_data.num_target_tokens-test_data.pad_length, pad_length=test_data.pad_length)\n",
    "loader_eval.dataset.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader_eval = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=False)\n",
    "# # loader_eval.dataset.eval()\n",
    "# # evaluate(model, loader_eval, tokenizer, ct# x, num_prefix_tokens=train_data.num_prefix_tokens, y_tokens=train_data.num_target_tokens-test_data.pad_length, pad_length=train_data.pad_length)\n",
    "loader_eval.dataset.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[3559,   11, 1433,  ...,   11, 1731,   11],\n",
       "         [3132,   11, 1065,  ...,   11, 1065,   11],\n",
       "         [1433,   11, 2091,  ...,   11, 2598,   11],\n",
       "         ...,\n",
       "         [1558,   11, 2231,  ...,   11, 1558,   11],\n",
       "         [  22,   11, 2091,  ...,   11, 1433,   11],\n",
       "         [2718,   11, 3682,  ...,   11, 2682,   11]], device='cuda:0'),\n",
       " tensor([[  -1,   -1,   -1,  ..., 1731,   11, 1983],\n",
       "         [  -1,   -1,   -1,  ..., 1065,   11,   18],\n",
       "         [  -1,   -1,   -1,  ..., 2598,   11, 2327],\n",
       "         ...,\n",
       "         [  -1,   -1,   -1,  ..., 1558,   11, 2231],\n",
       "         [  -1,   -1,   -1,  ..., 1433,   11,   20],\n",
       "         [  -1,   -1,   -1,  ..., 2682,   11, 1065]], device='cuda:0')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_loader.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(\n",
    "    x[:, :train_data.num_prefix_tokens],\n",
    "    num_beams=5,\n",
    "    max_new_tokens=9,\n",
    "    num_return_sequences=5,\n",
    "    temperature=1.0,\n",
    "    attention_mask=torch.ones_like(x[:, :train_data.num_prefix_tokens]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['15,35|35,5|20,11|16,24|20,15|11,16|24,2|5,31/20,31=20,15,35,5,31',\n",
       "       '15,35|35,5|20,11|16,24|20,15|11,16|24,2|5,31/20,31=20,11,16,24,2',\n",
       "       '15,35|35,5|20,11|16,24|20,15|11,16|24,2|5,31/20,31=20,5,31,35,5',\n",
       "       '15,35|35,5|20,11|16,24|20,15|11,16|24,2|5,31/20,31=20,15,35,5,30',\n",
       "       '15,35|35,5|20,11|16,24|20,15|11,16|24,2|5,31/20,31=20,25,35,5,31'],\n",
       "      dtype='<U69')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tokenizer.batch_decode(outputs)).reshape(x.shape[0], 5)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_model = deepcopy(model.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 64.00: 100%|██████████| 2/2 [00:03<00:00,  1.74s/it]: 98.44:  28%|██▊       | 9/32 [00:13<00:30,  1.31s/it] \n",
      "Epoch: [0/15] Loss: 0.1235 Verifier_loss: 0.36 Scores: 0.48 Acc: 99.50: 100%|██████████| 32/32 [00:45<00:00,  1.41s/it]\n",
      " accuracy: 55.00: 100%|██████████| 2/2 [00:03<00:00,  1.77s/it]: 100.00:  84%|████████▍ | 27/32 [00:36<00:06,  1.33s/it]\n",
      "Epoch: [1/15] Loss: 0.1292 Verifier_loss: 0.28 Scores: 0.49 Acc: 100.00: 100%|██████████| 32/32 [00:44<00:00,  1.40s/it]\n",
      "Epoch: [2/15] Loss: 0.1192 Verifier_loss: 0.27 Scores: 0.49 Acc: 100.00: 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\n",
      " accuracy: 53.00: 100%|██████████| 2/2 [00:03<00:00,  1.87s/it]: 100.00:  41%|████      | 13/32 [00:18<00:24,  1.31s/it]\n",
      "Epoch: [3/15] Loss: 0.1236 Verifier_loss: 0.26 Scores: 0.50 Acc: 100.00: 100%|██████████| 32/32 [00:45<00:00,  1.41s/it]\n",
      " accuracy: 54.00: 100%|██████████| 2/2 [00:03<00:00,  1.74s/it]: 100.00:  97%|█████████▋| 31/32 [00:41<00:01,  1.31s/it]\n",
      "Epoch: [4/15] Loss: 0.1242 Verifier_loss: 0.26 Scores: 0.49 Acc: 100.00: 100%|██████████| 32/32 [00:44<00:00,  1.40s/it]\n",
      "Epoch: [5/15] Loss: 0.1239 Verifier_loss: 0.26 Scores: 0.48 Acc: 100.00: 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\n",
      " accuracy: 55.00: 100%|██████████| 2/2 [00:03<00:00,  1.82s/it]: 100.00:  53%|█████▎    | 17/32 [00:23<00:19,  1.32s/it]\n",
      "Epoch: [6/15] Loss: 0.1272 Verifier_loss: 0.25 Scores: 0.49 Acc: 100.00: 100%|██████████| 32/32 [00:45<00:00,  1.41s/it]\n",
      "Epoch: [7/15] Loss: 0.1322 Verifier_loss: 0.25 Scores: 0.48 Acc: 100.00: 100%|██████████| 32/32 [00:41<00:00,  1.30s/it]\n",
      " accuracy: 57.00: 100%|██████████| 2/2 [00:03<00:00,  1.80s/it]: 100.00:   9%|▉         | 3/32 [00:05<00:38,  1.31s/it]\n",
      "Epoch: [8/15] Loss: 0.1287 Verifier_loss: 0.24 Scores: 0.49 Acc: 100.00: 100%|██████████| 32/32 [00:44<00:00,  1.41s/it]\n",
      " accuracy: 50.00: 100%|██████████| 2/2 [00:03<00:00,  1.75s/it]: 100.00:  66%|██████▌   | 21/32 [00:28<00:14,  1.31s/it]\n",
      "Epoch: [9/15] Loss: 0.1259 Verifier_loss: 0.24 Scores: 0.50 Acc: 100.00: 100%|██████████| 32/32 [00:44<00:00,  1.40s/it]\n"
     ]
    }
   ],
   "source": [
    "args.lr = 1e-6\n",
    "\n",
    "\n",
    "model = model.cuda()\n",
    "sft_model = sft_model.cuda()\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "# train_loader.batch_size = 16\n",
    "for ep in range(10):\n",
    "    train_bar = tqdm(train_loader)\n",
    "    total_loss, total_acc = AverageMeter(), AverageMeter()\n",
    "    verifier_loss = AverageMeter()\n",
    "    scores_meter = AverageMeter()\n",
    "    for x, y in train_bar:\n",
    "        # determine and set the learning rate for this iteration\n",
    "        lr = get_lr(num_iters, args.lr, warmup_iters, lr_decay_iters, min_lr) if decay_lr else args.lr\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        with ctx:\n",
    "            # logits, loss, accs = model(x, y)\n",
    "            logits = model(x)['logits']\n",
    "            loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "            acc = torch.mean((torch.argmax(logits[:, -train_data.num_target_tokens, :], dim=-1) == y[:, -train_data.num_target_tokens]).float())\n",
    "\n",
    "        if ep >= 0:\n",
    "            outputs = model.generate(\n",
    "                x[:, :train_data.num_prefix_tokens],\n",
    "                num_beams=2,\n",
    "                max_new_tokens=9,\n",
    "                num_return_sequences=2,\n",
    "                temperature=1.0,\n",
    "                attention_mask=torch.ones_like(x[:, :train_data.num_prefix_tokens]),\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            y_tokens=train_data.num_target_tokens-train_data.pad_length\n",
    "            scores = []\n",
    "            for i in range(outputs.shape[0]):\n",
    "                scores.append(int(tokenizer.decode(outputs[i, -y_tokens:]) in tokenizer.decode(y[i // 2, -y_tokens:])))\n",
    "            scores = torch.tensor(scores).float().cuda()\n",
    "            x_new = outputs[:, :-1].clone()\n",
    "            y_new = outputs.clone()\n",
    "            y_new[:, :train_data.num_prefix_tokens] = -1\n",
    "            y_new = y_new[:, 1:]\n",
    "            with ctx:\n",
    "                logits = model(x_new)['logits']\n",
    "            # print(logits.shape, y_new.shape)\n",
    "            vloss = torch.nn.functional.cross_entropy(logits.reshape(-1, logits.size(-1)), y_new.reshape(-1), ignore_index=-1, reduction='none')\n",
    "            with torch.no_grad():\n",
    "                with ctx:\n",
    "                    ref_logits = sft_model(x_new)['logits']\n",
    "                    vloss_ref = torch.nn.functional.cross_entropy(ref_logits.reshape(-1, ref_logits.size(-1)), y_new.reshape(-1), ignore_index=-1, reduction='none')\n",
    "            sign = (2 * scores - 1.) \n",
    "            sign = sign.unsqueeze(1).repeat(1, x_new.shape[-1])\n",
    "            vloss = vloss.reshape(x_new.shape[0], x_new.shape[1])\n",
    "            vloss_ref = vloss_ref.reshape(x_new.shape[0], x_new.shape[1])\n",
    "            # print(vloss.shape, sign.shape)\n",
    "            vloss_inside = ((vloss - vloss_ref) * sign).sum(1)\n",
    "            vloss = -torch.log(torch.sigmoid(vloss_inside))\n",
    "            vloss = vloss.mean()\n",
    "        else:\n",
    "            vloss = torch.tensor(0.)\n",
    "            scores = torch.tensor(0.)\n",
    "        verifier_loss.update(vloss_inside.item(), x.shape[0] * train_data.num_target_tokens)\n",
    "        scores_meter.update(scores.mean().item(), x.shape[0]*10)\n",
    "        total_loss.update(loss.item(), x.shape[0] * train_data.num_target_tokens)\n",
    "        total_acc.update(acc.item(), x.shape[0] * train_data.num_target_tokens)\n",
    "\n",
    "        scaler.scale(1. * loss + 1. * vloss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        num_iters += 1\n",
    "        \n",
    "        train_bar.set_description(\n",
    "            'Epoch: [{}/{}] Loss: {:.4f} Verifier_loss: {:.2f} Scores: {:.2f} Acc: {:.2f}'.format(ep, args.epochs_sft, total_loss.get(), verifier_loss.get(), scores_meter.get(),\n",
    "             total_acc.get(percentage=True))\n",
    "        )\n",
    "        if num_iters % 50 == 0:\n",
    "            loader_eval = torch.utils.data.DataLoader(test_data, batch_size=args.batch_size, shuffle=False)\n",
    "            loader_eval.dataset.eval()\n",
    "            evaluate(model, loader_eval, tokenizer, ctx, num_prefix_tokens=test_data.num_prefix_tokens, y_tokens=test_data.num_target_tokens-test_data.pad_length, pad_length=test_data.pad_length)\n",
    "            loader_eval.dataset.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6931)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-torch.log(torch.sigmoid(vloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['27,11|22,43|27,1|11,44|36,41|1,22|44,36|43,31/27,41=27,1,22,43,31',\n",
       "        '27,11|22,43|27,1|11,44|36,41|1,22|44,36|43,31/27,41=27,11,44,36,41'],\n",
       "       ['26,29|34,31|31,26|29,22|36,4|10,36|38,10|34,38/34,22=34,31,26,29,22',\n",
       "        '26,29|34,31|31,26|29,22|36,4|10,36|38,10|34,38/34,22=34,38,10,36,4'],\n",
       "       ['11,33|4,15|31,43|44,41|36,11|15,31|36,4|33,44/36,43=36,4,15,31,43',\n",
       "        '11,33|4,15|31,43|44,41|36,11|15,31|36,4|33,44/36,43=36,11,33,44,41'],\n",
       "       ['30,49|35,38|2,23|34,30|46,35|2,34|23,46|49,20/2,20=2,23,46,35,38',\n",
       "        '30,49|35,38|2,23|34,30|46,35|2,34|23,46|49,20/2,20=2,34,30,49,20'],\n",
       "       ['26,49|29,9|36,12|9,26|29,28|44,36|28,44|49,18/29,18=29,28,44,36,12',\n",
       "        '26,49|29,9|36,12|9,26|29,28|44,36|28,44|49,18/29,18=29,9,26,49,18'],\n",
       "       ['37,47|7,48|7,9|48,5|44,25|9,44|25,11|5,37/7,11=7,9,44,25,11',\n",
       "        '37,47|7,48|7,9|48,5|44,25|9,44|25,11|5,37/7,11=7,48,5,37,47'],\n",
       "       ['0,7|13,1|23,45|1,23|43,31|7,43|26,0|26,13/26,31=26,0,7,43,31',\n",
       "        '0,7|13,1|23,45|1,23|43,31|7,43|26,0|26,13/26,31=26,13,1,23,45'],\n",
       "       ['22,33|5,22|5,35|10,17|19,32|33,19|48,10|35,48/5,17=5,35,48,10,17',\n",
       "        '22,33|5,22|5,35|10,17|19,32|33,19|48,10|35,48/5,17=5,22,33,19,32']],\n",
       "      dtype='<U67')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tokenizer.batch_decode(outputs)).reshape(x.shape[0], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6,14|19,25|14,19|18,32|32,39|39,20|18,6|20,42/18,42=18,32,39,20,',\n",
       " '35,3|3,22|22,12|12,31|43,20|49,5|5,43|35,49/35,20=35,49,5,43,',\n",
       " '40,27|27,45|7,43|30,28|40,7|6,47|45,30|43,6/40,47=40,7,43,6,',\n",
       " '28,32|7,37|37,36|36,42|8,6|32,8|6,41|28,7/28,42=28,7,37,36,',\n",
       " '21,1|39,26|35,27|26,21|28,39|27,48|28,35|48,2/28,1=28,39,26,21,',\n",
       " '0,23|49,18|20,1|8,7|39,0|23,8|39,49|18,20/39,1=39,49,18,20,',\n",
       " '26,9|23,26|18,46|25,13|46,25|3,23|4,3|4,18/4,13=4,18,46,25,',\n",
       " '21,45|38,27|27,4|4,35|45,5|9,38|9,21|5,1/9,35=9,38,27,4,',\n",
       " '25,24|24,42|23,45|48,36|45,44|23,25|44,48|42,14/23,36=23,45,44,48,',\n",
       " '12,26|32,34|46,12|48,30|26,48|42,32|46,31|31,42/46,30=46,12,26,48,',\n",
       " '3,7|26,3|7,10|12,38|10,45|26,35|38,47|35,12/26,47=26,35,12,38,',\n",
       " '36,23|5,18|45,26|38,5|18,37|38,45|26,36|37,42/38,23=38,45,26,36,',\n",
       " '19,44|5,30|5,15|1,36|12,1|44,29|30,19|15,12/5,36=5,15,12,1,',\n",
       " '30,10|29,32|24,12|35,29|24,46|12,19|46,35|19,30/24,32=24,46,35,29,',\n",
       " '8,48|46,34|29,28|48,29|24,46|34,25|25,33|24,8/24,28=24,8,48,29,',\n",
       " '2,7|37,14|12,2|38,37|12,38|7,23|14,18|23,19/12,18=12,38,37,14,',\n",
       " '28,40|17,24|35,37|37,28|40,5|15,10|24,15|35,17/35,10=35,17,24,15,',\n",
       " '30,0|34,48|48,13|27,34|22,5|5,26|26,30|22,27/22,13=22,27,34,48,',\n",
       " '8,7|19,10|15,12|19,15|27,25|12,27|7,13|10,8/19,13=19,10,8,7,',\n",
       " '44,10|8,34|16,46|34,44|10,45|46,43|8,18|18,16/8,43=8,18,16,46,',\n",
       " '26,35|26,43|48,39|39,42|43,41|35,48|41,34|34,1/26,42=26,35,48,39,',\n",
       " '44,8|8,4|4,42|20,44|37,7|38,49|20,37|7,38/20,49=20,37,7,38,',\n",
       " '17,12|22,28|49,17|9,24|49,9|13,7|12,22|24,13/49,7=49,9,24,13,',\n",
       " '9,4|4,6|42,12|27,9|17,26|26,42|24,27|24,17/24,12=24,17,26,42,',\n",
       " '35,3|20,14|3,40|19,35|23,19|23,26|6,20|26,6/23,40=23,19,35,3,',\n",
       " '11,44|18,30|13,26|18,13|44,23|30,11|37,2|26,37/18,2=18,13,26,37,',\n",
       " '31,12|12,22|48,46|30,32|3,31|32,26|46,30|48,3/48,26=48,46,30,32,',\n",
       " '32,3|16,12|3,19|32,34|34,6|6,16|19,47|47,28/32,12=32,34,6,16,',\n",
       " '0,19|16,35|36,17|31,23|11,16|31,0|23,11|19,36/31,35=31,23,11,16,',\n",
       " '13,0|37,9|17,2|13,44|2,39|18,37|44,18|0,17/13,39=13,0,17,2,',\n",
       " '12,13|32,19|19,21|40,32|13,35|40,12|35,24|21,1/40,1=40,32,19,21,',\n",
       " '2,39|33,31|28,22|22,15|39,28|31,11|46,33|2,46/2,11=2,46,33,31,',\n",
       " '21,47|44,9|9,21|43,15|38,44|35,43|48,35|38,48/38,15=38,48,35,43,',\n",
       " '7,35|31,34|34,7|29,31|43,39|40,30|30,43|29,40/29,39=29,40,30,43,',\n",
       " '31,0|1,19|19,24|29,36|36,23|0,1|20,29|31,20/31,23=31,20,29,36,',\n",
       " '12,45|2,44|33,29|34,28|22,33|44,34|12,2|45,22/12,29=12,45,22,33,',\n",
       " '5,8|32,5|21,2|42,32|2,40|42,21|8,4|40,18/42,18=42,21,2,40,',\n",
       " '21,10|0,24|14,30|10,27|15,9|9,21|24,14|15,0/15,27=15,9,21,10,',\n",
       " '8,21|41,14|36,1|36,27|27,0|0,41|1,8|21,42/36,14=36,27,0,41,',\n",
       " '39,40|49,41|38,17|48,49|21,48|17,39|40,44|38,21/38,44=38,17,39,40,',\n",
       " '22,48|48,13|2,15|49,21|29,22|21,11|15,49|2,29/2,11=2,15,49,21,',\n",
       " '32,33|33,6|6,0|0,23|46,15|17,28|15,17|32,46/32,23=32,33,6,0,',\n",
       " '46,1|1,5|5,39|8,30|49,46|28,8|49,22|22,28/49,39=49,46,1,5,',\n",
       " '36,33|36,45|27,5|33,27|28,16|16,8|45,28|5,31/36,8=36,45,28,16,',\n",
       " '49,22|4,47|23,30|12,33|44,4|22,12|44,49|47,23/44,30=44,4,47,23,',\n",
       " '47,23|28,36|4,47|29,22|31,4|22,28|31,29|23,37/31,36=31,29,22,28,',\n",
       " '48,42|39,48|49,35|47,30|30,49|14,27|39,47|42,14/39,35=39,47,30,49,',\n",
       " '43,23|15,4|31,33|3,38|23,31|43,3|33,29|38,15/43,4=43,3,38,15,',\n",
       " '45,49|49,0|39,30|45,39|0,42|30,16|42,44|16,22/45,22=45,39,30,16,',\n",
       " '14,9|24,10|47,32|46,47|46,25|32,14|25,24|10,34/46,34=46,25,24,10,',\n",
       " '18,24|39,34|24,4|4,28|33,29|18,39|28,41|34,33/18,41=18,24,4,28,',\n",
       " '2,9|12,7|2,12|1,29|9,4|7,33|4,1|33,34/2,34=2,12,7,33,',\n",
       " '0,29|42,7|7,11|11,4|41,21|4,45|21,0|42,41/42,45=42,7,11,4,',\n",
       " '0,33|49,5|39,49|41,47|47,0|4,39|36,4|36,41/36,5=36,4,39,49,',\n",
       " '12,32|47,12|25,2|15,42|47,14|32,15|46,25|14,46/47,42=47,12,32,15,',\n",
       " '13,36|30,11|7,12|7,13|12,30|16,34|11,1|36,16/7,1=7,12,30,11,',\n",
       " '37,19|46,24|19,25|21,8|25,2|46,37|24,5|5,21/46,2=46,37,19,25,',\n",
       " '29,1|23,22|1,24|22,46|30,35|23,32|32,29|46,30/23,35=23,22,46,30,',\n",
       " '35,2|4,18|26,24|35,26|30,10|15,30|24,4|2,15/35,10=35,2,15,30,',\n",
       " '35,37|4,36|28,35|37,4|22,32|45,41|28,45|41,22/28,32=28,45,41,22,',\n",
       " '13,12|40,8|8,47|17,43|28,13|47,17|40,28|12,10/40,43=40,8,47,17,',\n",
       " '16,44|31,48|44,25|14,16|22,14|19,12|22,19|12,31/22,48=22,19,12,31,',\n",
       " '19,1|45,24|22,29|43,22|1,17|45,47|47,43|24,19/45,29=45,47,43,22,',\n",
       " '33,13|0,17|0,26|17,43|24,44|26,45|43,33|45,24/0,13=0,17,43,33,']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(next(test_loader.__iter__())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syth-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
