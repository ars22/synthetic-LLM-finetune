{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/asetlur/anaconda3/envs/syth-llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-20 05:02:35,403] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from data import get_dataset\n",
    "from utils.training_utils import get_lr, get_run_name, AverageMeter\n",
    "from torch.utils.data import DataLoader\n",
    "from models import get_model\n",
    "from tokenizing import get_tokenizer\n",
    "import wandb\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from copy import deepcopy\n",
    "from transformers import TrainingArguments\n",
    "from trl import DPOTrainer\n",
    "from transformers import AutoTokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Next-token failures\")\n",
    "# Data\n",
    "parser.add_argument(\n",
    "    \"--n_samples\", type=int, default=5, help=\"Number of samples to generate\"\n",
    "    )\n",
    "parser.add_argument(\n",
    "    \"--model\", default='gpt2', type=str, help=\"Type of model\"\n",
    "    )\n",
    "parser.add_argument(\n",
    "    \"--dataset\", default='graph', type=str, help=\"Choice of dataset\"\n",
    "    )\n",
    "parser.add_argument(\n",
    "    \"--n_train\", default=200000, type=int, help=\"Number of training samples\"\n",
    "    )\n",
    "parser.add_argument(\n",
    "    \"--n_test\", default=500, type=int, help=\"Number of test samples\"\n",
    "    )\n",
    "parser.add_argument(\n",
    "    \"--num_nodes\", default=50, type=int, help=\"Number of node values in graph\"\n",
    "    )\n",
    "parser.add_argument(\n",
    "    \"--deg\", default=2, type=int, help=\"Degree of starting node\"\n",
    "    )\n",
    "parser.add_argument(\n",
    "    \"--path_len\", default=5, type=int, help=\"Path length in star graph\"\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--mate_in\", default=2, type=int, help=\"For chess, number of moves to checkmate\"\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--unrolled\", action=argparse.BooleanOptionalAction, default=True, help=\"For chess, unrolled board state\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--batch_size\", type=int, default=48, help=\"Batch size\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--lr\", type=float, default=1e-5, help=\"Learning rate\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--weight_decay\", type=float, default=0., help=\"Strength of weight decay\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--epochs_sft\", type=int, default=1, help=\"Number of SFT epochs\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--save_every\", type=int, default=5000, help=\"Interval (in steps) at which to save model\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--pass_at_k\", type=int, default=1, help=\"pass at k eval\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--teacherless\", action=argparse.BooleanOptionalAction, default=False, help=\"Standard or teacherless training\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--reverse\", action=argparse.BooleanOptionalAction, default=False, help=\"Standard format or reverse targets\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--cot\", action=argparse.BooleanOptionalAction, default=False, help=\"Standard format or cot targets\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--pos\", action=argparse.BooleanOptionalAction, default=False, help=\"Standard format or pos tokens\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--eval_train\", action=argparse.BooleanOptionalAction, default=False, help=\"Eval for training set\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--eval_every\", type=int, default=400, help=\"Interval (in steps) to evaluate the model on test\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--use_wandb\", action=argparse.BooleanOptionalAction, default=False, help=\"Whether to use wandb\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "        \"--wandb_entity\", type=str, default=5000, help=\"Wandb username\",\n",
    "    )\n",
    "parser.add_argument(\n",
    "    \"--pad_length\", default=0, type=int, help=\"Default value for pad length\"\n",
    ")\n",
    "\n",
    "\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System stuff\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "wandb_entity = args.wandb_entity\n",
    "wandb_log = args.use_wandb\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "\n",
    "# Model stuff\n",
    "top_k = 1000\n",
    "temperature = 1.\n",
    "pass_at_k = args.pass_at_k\n",
    "n_samples = args.n_samples\n",
    "\n",
    "# Evaluation stuff\n",
    "eval_iters = 1000\n",
    "eval_interval = 5\n",
    "log_interval = 10\n",
    "\n",
    "# Optimiser\n",
    "dtype = 'bfloat16'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "decay_lr = False\n",
    "args.compile = False if device == 'cuda' else False\n",
    "args.use_flash = True if device == 'cuda' else False\n",
    "warmup_iters = 100\n",
    "min_lr = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equal sequence lengths!\n",
      "Equal sequence lengths!\n",
      "Equal sequence lengths!\n",
      "Equal sequence lengths!\n",
      "Equal sequence lengths!\n",
      "1000 torch.Size([54]) 300 torch.Size([54])\n",
      "W/o COT (tensor([ 2920,    11,  1485,    91,  1954,    11,  2078,    91,  3510,    11,\n",
      "         2327,    91,  2624,    11,  3682,    91,  2327,    11,  2624,    91,\n",
      "         3682,    11,  1828,    91,  1485,    11,  1954,    91,  3510,    11,\n",
      "         2920,    14,  3510,    11,  1828,    28,  3510,    11,  2327,    11,\n",
      "         2624,    11,  3682,    11,  1828, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256], device='cuda:0'), tensor([   -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "           -1,    -1,    -1,    -1,    -1,  3510,    11,  2327,    11,  2624,\n",
      "           11,  3682,    11,  1828, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "        50256, 50256, 50256, 50256], device='cuda:0')) 49,13|23,28|46,35|32,42|35,32|42,22|13,23|46,49/46,22=46,35,32,42,22<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
      "With COT (tensor([1270,   11, 2327,   91,   17,   11, 1270,   91, 2920,   11, 1959,   91,\n",
      "        1314,   11,   21,   91, 1314,   11, 2920,   91, 1731,   11, 1433,   91,\n",
      "          21,   11,   17,   91, 1959,   11, 1731,   14, 1314,   11, 1433,   28,\n",
      "        1433,   11, 1731,   11, 1959,   11, 2920,   11, 1314,   25, 1314,   11,\n",
      "        2920,   11, 1959,   11, 1731,   11], device='cuda:0'), tensor([  -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "          -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,\n",
      "          -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1, 1433,\n",
      "          11, 1731,   11, 1959,   11, 2920,   11, 1314,   25, 1314,   11, 2920,\n",
      "          11, 1959,   11, 1731,   11, 1433], device='cuda:0')) 30,35|2,30|49,29|15,6|15,49|24,16|6,2|29,24/15,16=16,24,29,49,15:15,49,29,24,\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenizer(args)\n",
    "\n",
    "# train data without cot\n",
    "args.cot = False\n",
    "args.n_train = 1000\n",
    "args.n_test = 100\n",
    "args.pad_length = 10\n",
    "train_data, test_data = get_dataset(args, tokenizer, device)\n",
    "train_data.num_target_tokens += train_data.pad_length\n",
    "test_data.num_target_tokens += test_data.pad_length\n",
    "train_data.num_tokens += train_data.pad_length\n",
    "test_data.num_tokens += test_data.pad_length\n",
    "\n",
    "# train data with cot\n",
    "args.cot = True\n",
    "args.n_train = 300\n",
    "args.pad_length = 0\n",
    "train_data_wcot, _ = get_dataset(args, tokenizer, device)\n",
    "train_data_wcot.data_file = train_data_wcot.data_file[-args.n_train:]\n",
    "train_data_wcot.tokenized, train_data_wcot.num_prefix_tokens, train_data_wcot.num_target_tokens = train_data_wcot.tokenizer.tokenize(train_data_wcot.data_file)\n",
    "\n",
    "\n",
    "print(len(train_data), train_data[0][0].shape, len(train_data_wcot), train_data_wcot[0][0].shape)\n",
    "print(\"W/o COT\", train_data[0], tokenizer.decode(train_data[0][0]))\n",
    "print(\"With COT\", train_data_wcot[0], tokenizer.decode(train_data_wcot[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36, 19, 55, 36, 19, 55, 36, 19, 55)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.num_prefix_tokens, train_data.num_target_tokens, train_data.num_tokens, train_data_wcot.num_prefix_tokens, train_data_wcot.num_target_tokens, train_data_wcot.num_tokens, test_data.num_prefix_tokens, test_data.num_target_tokens, test_data.num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "concatenated_train_data = ConcatDataset([train_data, train_data_wcot])\n",
    "train_loader = DataLoader(concatenated_train_data, batch_size=args.batch_size, shuffle=True)\n",
    "# train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=args.batch_size, shuffle=True)\n",
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, tokenizer, ctx, num_prefix_tokens, y_tokens, num_target_tokens):\n",
    "    model.eval()\n",
    "    total_acc = AverageMeter()\n",
    "    bar = tqdm(loader)\n",
    "    pad_length = num_target_tokens - y_tokens\n",
    "    c = 0\n",
    "    # print(pad_length)\n",
    "    for x in bar:\n",
    "        y = x[:, num_prefix_tokens:].clone()\n",
    "        x = x[:, :num_prefix_tokens].clone()\n",
    "        with ctx:\n",
    "            y_pred = model.generate(x, max_new_tokens=num_target_tokens,\n",
    "                                do_sample=False, attention_mask = torch.ones_like(x), pad_token_id=2)\n",
    "        completely_correct = 0 \n",
    "        for i in range(y.shape[0]):\n",
    "            # if c == 0 and i == 0:    \n",
    "                # print(\"y:\", tokenizer.decode(y[i, -y_tokens:]), \"ypred:\", tokenizer.decode(y_pred[i, -y_tokens:]))\n",
    "                # print(\"y:\", tokenizer.decode(y[i, -y_tokens:]), \"ypred:\", tokenizer.decode(y_pred[i, -y_tokens-pad_length:-pad_length]))\n",
    "                # print(\"y_pred:\", tokenizer.decode(y_pred[i]), \"y:\", tokenizer.decode(y[i]))\n",
    "            completely_correct += int(tokenizer.decode(y[i]) in tokenizer.decode(y_pred[i]))\n",
    "        completely_correct /= x.shape[0]\n",
    "        total_acc.update(completely_correct, x.shape[0])\n",
    "        c += 1\n",
    "        bar.set_description(f' accuracy: {total_acc.get(percentage=True):.2f}')\n",
    "    loader.dataset.train()\n",
    "    model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.num_prefix_tokens + train_data.num_target_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20] Loss: 0.7631 Verifier_loss: 0.00 Acc: 78.21: 100%|██████████| 28/28 [00:06<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed\n",
      "Train without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 54.80: 100%|██████████| 21/21 [00:09<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 0.00: 100%|██████████| 7/7 [00:02<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 46.00: 100%|██████████| 3/3 [00:01<00:00,  2.75it/s]\n",
      "Epoch: [1/20] Loss: 0.2399 Verifier_loss: 0.00 Acc: 85.15: 100%|██████████| 28/28 [00:06<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed\n",
      "Train without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 62.10: 100%|██████████| 21/21 [00:08<00:00,  2.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 0.67: 100%|██████████| 7/7 [00:02<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 48.00: 100%|██████████| 3/3 [00:01<00:00,  2.33it/s]\n",
      "Epoch: [2/20] Loss: 0.1744 Verifier_loss: 0.00 Acc: 87.80: 100%|██████████| 28/28 [00:06<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed\n",
      "Train without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 68.90: 100%|██████████| 21/21 [00:08<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 1.67: 100%|██████████| 7/7 [00:03<00:00,  2.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 52.00: 100%|██████████| 3/3 [00:01<00:00,  2.77it/s]\n",
      "Epoch: [3/20] Loss: 0.1433 Verifier_loss: 0.00 Acc: 89.37: 100%|██████████| 28/28 [00:06<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 completed\n",
      "Train without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 76.50: 100%|██████████| 21/21 [00:09<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 4.67: 100%|██████████| 7/7 [00:03<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 51.00: 100%|██████████| 3/3 [00:01<00:00,  2.60it/s]\n",
      "Epoch: [4/20] Loss: 0.1169 Verifier_loss: 0.00 Acc: 91.03: 100%|██████████| 28/28 [00:06<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 completed\n",
      "Train without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 80.90: 100%|██████████| 21/21 [00:08<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 18.00: 100%|██████████| 7/7 [00:03<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 49.00: 100%|██████████| 3/3 [00:01<00:00,  2.80it/s]\n",
      "Epoch: [5/20] Loss: 0.1010 Verifier_loss: 0.00 Acc: 92.37: 100%|██████████| 28/28 [00:06<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 completed\n",
      "Train without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 85.70: 100%|██████████| 21/21 [00:08<00:00,  2.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 37.33: 100%|██████████| 7/7 [00:03<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 46.00: 100%|██████████| 3/3 [00:01<00:00,  2.74it/s]\n",
      "Epoch: [6/20] Loss: 0.0693 Verifier_loss: 0.00 Acc: 94.82: 100%|██████████| 28/28 [00:06<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 completed\n",
      "Train without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 91.40: 100%|██████████| 21/21 [00:08<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 64.00: 100%|██████████| 7/7 [00:03<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 56.00: 100%|██████████| 3/3 [00:01<00:00,  2.30it/s]\n",
      "Epoch: [7/20] Loss: 0.0645 Verifier_loss: 0.00 Acc: 95.03: 100%|██████████| 28/28 [00:06<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 completed\n",
      "Train without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 92.80: 100%|██████████| 21/21 [00:08<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 62.67: 100%|██████████| 7/7 [00:03<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 48.00: 100%|██████████| 3/3 [00:01<00:00,  2.84it/s]\n",
      "Epoch: [8/20] Loss: 0.0480 Verifier_loss: 0.00 Acc: 96.25: 100%|██████████| 28/28 [00:06<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 completed\n",
      "Train without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 95.50: 100%|██████████| 21/21 [00:07<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 55.67: 100%|██████████| 7/7 [00:02<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 49.00: 100%|██████████| 3/3 [00:01<00:00,  2.91it/s]\n",
      "Epoch: [9/20] Loss: 0.0403 Verifier_loss: 0.00 Acc: 96.91: 100%|██████████| 28/28 [00:06<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 completed\n",
      "Train without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 96.20: 100%|██████████| 21/21 [00:07<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 72.33: 100%|██████████| 7/7 [00:03<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 54.00: 100%|██████████| 3/3 [00:01<00:00,  2.71it/s]\n",
      "Epoch: [10/20] Loss: 0.0487 Verifier_loss: 0.00 Acc: 96.29: 100%|██████████| 28/28 [00:06<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 completed\n",
      "Train without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 95.30: 100%|██████████| 21/21 [00:08<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 77.67: 100%|██████████| 7/7 [00:03<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 57.00: 100%|██████████| 3/3 [00:01<00:00,  2.69it/s]\n",
      "Epoch: [11/20] Loss: 0.0360 Verifier_loss: 0.00 Acc: 97.32: 100%|██████████| 28/28 [00:06<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 completed\n",
      "Train without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 96.60: 100%|██████████| 21/21 [00:06<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 83.00: 100%|██████████| 7/7 [00:03<00:00,  2.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 62.00: 100%|██████████| 3/3 [00:01<00:00,  2.69it/s]\n",
      "Epoch: [12/20] Loss: 0.0299 Verifier_loss: 0.00 Acc: 97.81: 100%|██████████| 28/28 [00:06<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 completed\n",
      "Train without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 98.60: 100%|██████████| 21/21 [00:07<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 93.33: 100%|██████████| 7/7 [00:03<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 61.00: 100%|██████████| 3/3 [00:01<00:00,  2.67it/s]\n",
      "Epoch: [13/20] Loss: 0.0283 Verifier_loss: 0.00 Acc: 97.80: 100%|██████████| 28/28 [00:06<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 completed\n",
      "Train without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 97.80: 100%|██████████| 21/21 [00:07<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 92.33: 100%|██████████| 7/7 [00:03<00:00,  2.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 60.00: 100%|██████████| 3/3 [00:01<00:00,  2.27it/s]\n",
      "Epoch: [14/20] Loss: 0.0249 Verifier_loss: 0.00 Acc: 98.26: 100%|██████████| 28/28 [00:06<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 completed\n",
      "Train without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 99.20: 100%|██████████| 21/21 [00:06<00:00,  3.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 97.67: 100%|██████████| 7/7 [00:03<00:00,  2.33it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 60.00: 100%|██████████| 3/3 [00:01<00:00,  2.64it/s]\n",
      "Epoch: [15/20] Loss: 0.0187 Verifier_loss: 0.00 Acc: 98.68: 100%|██████████| 28/28 [00:06<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 completed\n",
      "Train without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 99.40: 100%|██████████| 21/21 [00:07<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 95.33: 100%|██████████| 7/7 [00:02<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 61.00: 100%|██████████| 3/3 [00:01<00:00,  2.29it/s]\n",
      "Epoch: [16/20] Loss: 0.0148 Verifier_loss: 0.00 Acc: 98.97: 100%|██████████| 28/28 [00:06<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 completed\n",
      "Train without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 99.70: 100%|██████████| 21/21 [00:06<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 97.67: 100%|██████████| 7/7 [00:03<00:00,  2.26it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 58.00: 100%|██████████| 3/3 [00:01<00:00,  2.76it/s]\n",
      "Epoch: [17/20] Loss: 0.0114 Verifier_loss: 0.00 Acc: 99.23: 100%|██████████| 28/28 [00:06<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 completed\n",
      "Train without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 99.70: 100%|██████████| 21/21 [00:07<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 98.67: 100%|██████████| 7/7 [00:03<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 66.00: 100%|██████████| 3/3 [00:01<00:00,  2.73it/s]\n",
      "Epoch: [18/20] Loss: 0.0104 Verifier_loss: 0.00 Acc: 99.26: 100%|██████████| 28/28 [00:06<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 completed\n",
      "Train without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 99.80: 100%|██████████| 21/21 [00:06<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 99.67: 100%|██████████| 7/7 [00:03<00:00,  2.31it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 62.00: 100%|██████████| 3/3 [00:01<00:00,  2.81it/s]\n",
      "Epoch: [19/20] Loss: 0.0127 Verifier_loss: 0.00 Acc: 99.15: 100%|██████████| 28/28 [00:06<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 completed\n",
      "Train without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 99.80: 100%|██████████| 21/21 [00:06<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train with COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 99.33: 100%|██████████| 7/7 [00:03<00:00,  2.26it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test without COT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " accuracy: 64.00: 100%|██████████| 3/3 [00:01<00:00,  2.75it/s]\n"
     ]
    }
   ],
   "source": [
    "args.model = 'gpt2-medium'\n",
    "args.lr = 1e-4\n",
    "args.epochs_sft = 20\n",
    "from transformers import GPT2LMHeadModel\n",
    "model = GPT2LMHeadModel.from_pretrained(args.model)\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=0.0)\n",
    "ctx = nullcontext() if device == 'cpu' else torch.amp.autocast(device_type=device, dtype=ptdtype)\n",
    "\n",
    "max_iters = len(train_loader) * args.epochs_sft\n",
    "lr_decay_iters = max_iters\n",
    "\n",
    "results = {}\n",
    "num_iters = 0\n",
    "\n",
    "for ep in range(args.epochs_sft):\n",
    "    train_bar = tqdm(train_loader)\n",
    "    total_loss, total_acc = AverageMeter(), AverageMeter()\n",
    "    verifier_loss = AverageMeter()\n",
    "    for x, y in train_bar:\n",
    "        with ctx:\n",
    "            # logits, loss, accs = model(x, y)\n",
    "            y_tokens = train_data.num_target_tokens - train_data.pad_length\n",
    "            logits = model(x)['logits']\n",
    "            # print(x.shape, y.shape, logits.shape)\n",
    "            loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "            acc = torch.mean((torch.argmax(logits[:, -y_tokens-train_data.pad_length:-train_data.pad_length, :], dim=-1) == y[:, -y_tokens-train_data.pad_length:-train_data.pad_length]).float())\n",
    "            \n",
    "            # if num_iters % 100 == 0:\n",
    "            #     print(tokenizer.decode(y[0, -y_tokens-train_data.pad_length:]), tokenizer.decode(torch.argmax(logits[0, -y_tokens-train_data.pad_length:, :], dim=-1)))\n",
    "            \n",
    "        # verifier \n",
    "        if ep > 5000:\n",
    "            num_prefix_tokens=train_data.num_prefix_tokens\n",
    "            num_target_tokens=train_data.num_target_tokens\n",
    "            y_tokens=train_data.num_target_tokens-train_data.pad_length\n",
    "            x = torch.cat([x, y[:, -1:]], dim=1)\n",
    "            y = x[:, -y_tokens:].clone()\n",
    "            x = x[:, :num_prefix_tokens].clone()\n",
    "            y_pred = []\n",
    "            scores = []\n",
    "            for i in range(args.n_samples):\n",
    "                with ctx:\n",
    "                    _y_pred = model.generate(x, min_length=num_prefix_tokens+num_target_tokens, \n",
    "                                max_length=num_prefix_tokens+num_target_tokens, temperature=1., top_p=0.99,\n",
    "                                do_sample=True, attention_mask = torch.ones_like(x), pad_token_id=2)\n",
    "                _scores = []\n",
    "                for i in range(y.shape[0]):\n",
    "                    _scores.append(int(tokenizer.decode(y[i]) in tokenizer.decode(_y_pred[i])))\n",
    "                y_pred.append(_y_pred)\n",
    "                scores.append(_scores)\n",
    "            scores = torch.tensor(scores).float().transpose(0, 1)\n",
    "            y_pred = torch.stack(y_pred, dim=0).transpose(0, 1)\n",
    "            y_pred = y_pred.reshape(-1, y_pred.shape[-1]) \n",
    "            sign = scores.reshape(-1) * 2 - 1.\n",
    "            # sign = scores.reshape(-1)\n",
    "            # sign = scores.reshape(-1) - 1. \n",
    "        \n",
    "            x = x.unsqueeze(0).repeat(args.n_samples, 1, 1).transpose(0, 1).reshape(-1, x.shape[-1])\n",
    "            x_new = torch.cat([x, y_pred], dim=1)\n",
    "            y_new = x_new.clone()\n",
    "            y_new[:, :x.shape[1]] = -1\n",
    "            x_new = x_new[:, :-1].clone()\n",
    "            y_new = y_new[:, 1:].clone()\n",
    "            sign = sign.cuda()\n",
    "            with ctx:\n",
    "                logits = model(x_new)['logits']\n",
    "                vloss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), y_new.view(-1), ignore_index=-1, reduction='none')\n",
    "                # print(vloss)\n",
    "                # vloss.sigmoid_()\n",
    "                vloss = torch.sigmoid(0.1 * vloss)\n",
    "                sign = sign.unsqueeze(1).repeat(1, x_new.shape[-1]).reshape(-1)\n",
    "                vloss = vloss * sign\n",
    "                vloss = vloss.mean()\n",
    "        else:\n",
    "            vloss = torch.tensor(0.)\n",
    "        verifier_loss.update(vloss.item(), x.shape[0] * train_data.num_target_tokens)\n",
    "        total_loss.update(loss.item(), x.shape[0] * train_data.num_target_tokens)\n",
    "        total_acc.update(acc.item(), x.shape[0] * train_data.num_target_tokens)\n",
    "        scaler.scale(loss + 0.1 * vloss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        num_iters += 1\n",
    "        train_bar.set_description(\n",
    "            'Epoch: [{}/{}] Loss: {:.4f} Verifier_loss: {:.2f} Acc: {:.2f}'.format(ep, args.epochs_sft, total_loss.get(), verifier_loss.get(),\n",
    "             total_acc.get(percentage=True))\n",
    "        )\n",
    "        verifier_loss = AverageMeter()\n",
    "                \n",
    "\n",
    "    # evaluate the loss on train/val sets and write checkpoints\n",
    "    print(f\"Epoch {ep} completed\")\n",
    "    print(\"Train without COT\")\n",
    "    loader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=False)\n",
    "    loader.dataset.eval()\n",
    "    evaluate(model, loader, tokenizer, ctx, num_prefix_tokens=train_data.num_prefix_tokens, y_tokens=train_data.num_target_tokens-train_data.pad_length, num_target_tokens=train_data.num_target_tokens)\n",
    "    loader.dataset.train()\n",
    "    print(\"Train with COT\")\n",
    "    loader_wcot = torch.utils.data.DataLoader(train_data_wcot, batch_size=args.batch_size, shuffle=False)\n",
    "    loader_wcot.dataset.eval()\n",
    "    evaluate(model, loader_wcot, tokenizer, ctx, num_prefix_tokens=train_data_wcot.num_prefix_tokens, y_tokens=train_data_wcot.num_target_tokens-train_data.pad_length, num_target_tokens=train_data_wcot.num_target_tokens)\n",
    "    loader_wcot.dataset.train()\n",
    "    print(\"Test without COT\")\n",
    "    loader_eval = torch.utils.data.DataLoader(test_data, batch_size=args.batch_size, shuffle=False)\n",
    "    loader_eval.dataset.eval()\n",
    "    evaluate(model, loader_eval, tokenizer, ctx, num_prefix_tokens=test_data.num_prefix_tokens, y_tokens=test_data.num_target_tokens-test_data.pad_length, num_target_tokens=test_data.num_target_tokens)\n",
    "    loader_eval.dataset.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['49,13|23,28|46,35|32,42|35,32|42,22|13,23|46,49/46,22=46,35,32,42,22<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       "        '49,13|23,28|46,35|32,42|35,32|42,22|13,23|46,49/46,22=46,49,13,23,28,32,42,22<|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       "        '49,13|23,28|46,35|32,42|35,32|42,22|13,23|46,49/46,22=46,34,28,23,28,23,28,23,28,23',\n",
       "        '49,13|23,28|46,35|32,42|35,32|42,22|13,23|46,49/46,22=46,34,37,28,23,46:46,34,37,28',\n",
       "        '49,13|23,28|46,35|32,42|35,32|42,22|13,23|46,49/46,22=22,42,32,35,46:46,35,32,42,22']],\n",
       "      dtype='<U198')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(torch.utils.data.DataLoader(train_data)))\n",
    "outputs = model.generate(\n",
    "    x[:, :train_data.num_prefix_tokens],\n",
    "    num_beams=100,\n",
    "    max_new_tokens=19,\n",
    "    num_return_sequences=5,\n",
    "    temperature=1.0,\n",
    "    attention_mask=torch.ones_like(x[:, :train_data.num_prefix_tokens]),\n",
    ")\n",
    "np.array(tokenizer.batch_decode(outputs)).reshape(x.shape[0], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 1]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = []\n",
    "num_return_sequences = 5\n",
    "pad_length = train_data.pad_length\n",
    "for i in range(outputs.shape[0]):\n",
    "    y_actual = tokenizer.decode(y[i // num_return_sequences, -y_tokens-pad_length:-pad_length]) if y[i // num_return_sequences, -1] == tokenizer.pad_token_id else tokenizer.decode(y[i // num_return_sequences, -y_tokens:])\n",
    "    y_cot = \",\".join(y_actual.split(\",\")[::-1]) + \":\" + y_actual\n",
    "    scores.append(max(int(y_actual in tokenizer.decode(outputs[i])), int(y_cot in tokenizer.decode(outputs[i]))))\n",
    "    # print(y_actual, y_cot)\n",
    "    # scores.append(\n",
    "    #     int(tokenizer.decode(y[i // num_return_sequences, -y_tokens-pad_length:-pad_length]) in tokenizer.decode(outputs[i])) if tokenizer.decode(y[i // num_return_sequences, -1]) == tokenizer.pad_token_id else\n",
    "    #     int(tokenizer.decode(y[i // num_return_sequences, -y_tokens-pad_length:]) in tokenizer.decode(outputs[i])),\n",
    "    # )\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['30,35|2,30|49,29|15,6|15,49|24,16|6,2|29,24/15,16=16,24,29,49,15:15,49,29,24,16',\n",
       "        '30,35|2,30|49,29|15,6|15,49|24,16|6,2|29,24/15,16=15,49,29,24,16<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       "        '30,35|2,30|49,29|15,6|15,49|24,16|6,2|29,24/15,16=15,30,35,2,30,35,2,30,35,2',\n",
       "        '30,35|2,30|49,29|15,6|15,49|24,16|6,2|29,24/15,16=15,6,2,30,35<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>',\n",
       "        '30,35|2,30|49,29|15,6|15,49|24,16|6,2|29,24/15,16=15,24,16,29,49,15:15,49,29,24']],\n",
       "      dtype='<U194')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(torch.utils.data.DataLoader(train_data_wcot)))\n",
    "outputs = model.generate(\n",
    "    x[:, :train_data.num_prefix_tokens],\n",
    "    num_beams=100,\n",
    "    max_new_tokens=19,\n",
    "    num_return_sequences=5,\n",
    "    temperature=1.0,\n",
    "    attention_mask=torch.ones_like(x[:, :train_data.num_prefix_tokens]),\n",
    ")\n",
    "np.array(tokenizer.batch_decode(outputs)).reshape(x.shape[0], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15,49,29,24,16 16,24,29,49,15:15,49,29,24,16\n",
      "15,49,29,24,16 16,24,29,49,15:15,49,29,24,16\n",
      "15,49,29,24,16 16,24,29,49,15:15,49,29,24,16\n",
      "15,49,29,24,16 16,24,29,49,15:15,49,29,24,16\n",
      "15,49,29,24,16 16,24,29,49,15:15,49,29,24,16\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 0, 0]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = []\n",
    "num_return_sequences = 5\n",
    "pad_length = train_data.pad_length\n",
    "for i in range(outputs.shape[0]):\n",
    "    y_actual = tokenizer.decode(y[i // num_return_sequences, -y_tokens-pad_length:-pad_length]) if y[i // num_return_sequences, -1] == tokenizer.pad_token_id else tokenizer.decode(y[i // num_return_sequences, -y_tokens:])\n",
    "    y_cot = \",\".join(y_actual.split(\",\")[::-1]) + \":\" + y_actual\n",
    "    scores.append(max(int(y_actual in tokenizer.decode(outputs[i])), int(y_cot in tokenizer.decode(outputs[i]))))\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_model = deepcopy(model).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: [0/20] Loss: 0.4924 Verifier_loss: 21.59 Scores: 0.04 Acc: 95.52:  84%|████████▍ | 53/63 [01:05<00:12,  1.24s/it]"
     ]
    }
   ],
   "source": [
    "args.lr = 1e-5\n",
    "\n",
    "model = model.cuda()\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "for ep in range(10):\n",
    "    ref_model = deepcopy(model).cuda()\n",
    "    train_bar = tqdm(train_loader)\n",
    "    total_loss, total_acc = AverageMeter(), AverageMeter()\n",
    "    verifier_loss = AverageMeter()\n",
    "    scores_meter = AverageMeter()\n",
    "    for x, y in train_bar:\n",
    "        with ctx:\n",
    "            # logits, loss, accs = model(x, y)\n",
    "            logits = model(x)['logits']\n",
    "            loss = torch.nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1), ignore_index=-1)\n",
    "            acc = torch.mean((torch.argmax(logits[:, -train_data.num_target_tokens, :], dim=-1) == y[:, -train_data.num_target_tokens]).float())\n",
    "\n",
    "        num_return_sequences = 5\n",
    "        pad_length = train_data.pad_length\n",
    "        if ep >= 0:\n",
    "            outputs = model.generate(\n",
    "                x[:, :train_data.num_prefix_tokens],\n",
    "                num_beams=5,\n",
    "                max_new_tokens=19,\n",
    "                num_return_sequences=num_return_sequences,\n",
    "                temperature=1.0,\n",
    "                attention_mask=torch.ones_like(x[:, :train_data.num_prefix_tokens]),\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            scores = []\n",
    "            for i in range(outputs.shape[0]):\n",
    "                y_actual = tokenizer.decode(y[i // num_return_sequences, -y_tokens-pad_length:-pad_length]) if y[i // num_return_sequences, -1] == tokenizer.pad_token_id else tokenizer.decode(y[i // num_return_sequences, -y_tokens:])\n",
    "                y_cot = \",\".join(y_actual.split(\",\")[::-1]) + \":\" + y_actual\n",
    "                scores.append(max(int(y_actual in tokenizer.decode(outputs[i])), int(y_cot in tokenizer.decode(outputs[i]))))\n",
    "            scores = torch.tensor(scores).float().cuda()\n",
    "            x_new = outputs[:, :-1].clone()\n",
    "            y_new = outputs.clone()\n",
    "            y_new[:, :train_data.num_prefix_tokens] = -1\n",
    "            y_new = y_new[:, 1:]\n",
    "            with ctx:\n",
    "                logits = model(x_new)['logits']\n",
    "            vloss = torch.nn.functional.cross_entropy(logits.reshape(-1, logits.size(-1)), y_new.reshape(-1), ignore_index=-1, reduction='none')\n",
    "            with torch.no_grad():\n",
    "                with ctx:\n",
    "                    ref_logits = ref_model(x_new)['logits']\n",
    "                    vloss_ref = torch.nn.functional.cross_entropy(ref_logits.reshape(-1, ref_logits.size(-1)), y_new.reshape(-1), ignore_index=-1, reduction='none')\n",
    "            sign = (2 * scores - 1.) \n",
    "            sign = sign.unsqueeze(1).repeat(1, x_new.shape[-1])\n",
    "            vloss = vloss.reshape(x_new.shape[0], x_new.shape[1])\n",
    "            vloss_ref = vloss_ref.reshape(x_new.shape[0], x_new.shape[1])\n",
    "            # print(vloss.shape, vloss_ref.shape, sign.shape)\n",
    "            vloss_inside = ((vloss - vloss_ref) * sign).sum(1)\n",
    "            vloss = -torch.log(torch.sigmoid(vloss_inside))\n",
    "            vloss = vloss.mean()\n",
    "        else:\n",
    "            vloss = torch.tensor(0.)\n",
    "            scores = torch.tensor(0.)\n",
    "        verifier_loss.update(vloss.item(), x.shape[0] * train_data.num_target_tokens)\n",
    "        scores_meter.update(scores.mean().item(), x.shape[0]*num_return_sequences)\n",
    "        total_loss.update(loss.item(), x.shape[0] * train_data.num_target_tokens)\n",
    "        total_acc.update(acc.item(), x.shape[0] * train_data.num_target_tokens)\n",
    "\n",
    "        scaler.scale(1. * loss + 1. * vloss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        num_iters += 1\n",
    "        \n",
    "        train_bar.set_description(\n",
    "            'Epoch: [{}/{}] Loss: {:.4f} Verifier_loss: {:.2f} Scores: {:.2f} Acc: {:.2f}'.format(ep, args.epochs_sft, total_loss.get(), verifier_loss.get(), scores_meter.get(),\n",
    "             total_acc.get(percentage=True))\n",
    "        )\n",
    "        \n",
    "    print(f\"Epoch {ep} completed\")\n",
    "    print(\"Train without COT\")\n",
    "    loader = torch.utils.data.DataLoader(train_data, batch_size=args.batch_size, shuffle=False)\n",
    "    loader.dataset.eval()\n",
    "    evaluate(model, loader, tokenizer, ctx, num_prefix_tokens=train_data.num_prefix_tokens, y_tokens=train_data.num_target_tokens-train_data.pad_length, num_target_tokens=train_data.num_target_tokens)\n",
    "    loader.dataset.train()\n",
    "    print(\"Train with COT\")\n",
    "    loader_wcot = torch.utils.data.DataLoader(train_data_wcot, batch_size=args.batch_size, shuffle=False)\n",
    "    loader_wcot.dataset.eval()\n",
    "    evaluate(model, loader_wcot, tokenizer, ctx, num_prefix_tokens=train_data_wcot.num_prefix_tokens, y_tokens=train_data_wcot.num_target_tokens-train_data.pad_length, num_target_tokens=train_data_wcot.num_target_tokens)\n",
    "    loader_wcot.dataset.train()\n",
    "    print(\"Test without COT\")\n",
    "    loader_eval = torch.utils.data.DataLoader(test_data, batch_size=args.batch_size, shuffle=False)\n",
    "    loader_eval.dataset.eval()\n",
    "    evaluate(model, loader_eval, tokenizer, ctx, num_prefix_tokens=test_data.num_prefix_tokens, y_tokens=test_data.num_target_tokens-test_data.pad_length, num_target_tokens=test_data.num_target_tokens)\n",
    "    loader_eval.dataset.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syth-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
