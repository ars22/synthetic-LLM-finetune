{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/asetlur/anaconda3/envs/syth-llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import hydra \n","import transformers\n","import os\n","from peft import LoraConfig, get_peft_model\n","from pathlib import Path\n","from omegaconf import OmegaConf\n","import json\n","import transformers\n","import torch\n","import os\n","import json\n","import random\n","import numpy as np\n","import argparse\n","from typing import Dict, List, Optional, Iterator, Callable, Union, Tuple\n","import datasets\n","from torch.nn.utils.rnn import pad_sequence\n","from tqdm import tqdm\n","import sys\n","import re\n","from rouge_score import rouge_scorer\n","from collections import defaultdict\n","import re\n","from copy import  deepcopy\n","from torch.utils.data import Dataset"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["configs = {\n","    'question_start_tag': \"[INST] \",\n","    \"question_end_tag\": \" End your answer as, 'Hence the answer is: '. [/INST]\",\n","    \"answer_tag\": \"\",\n","    'max_length': 512,\n","    'model_id': \"mistralai/Mistral-7B-Instruct-v0.1\",\n","    'batch_size': 64\n","}"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["def delete_extra_zero(n):\n","    try:\n","        n=float(n)\n","    except:\n","        # print(\"None {}\".format(n))\n","        return n\n","    if isinstance(n, int):\n","        return str(n)\n","    if isinstance(n, float):\n","        n = str(n).rstrip('0')  \n","        n = int(n.rstrip('.')) if n.endswith('.') else float(n)  \n","        n=str(n)\n","        return n\n","    \n","    \n","def read_jsonl(path: str):\n","    with open(path) as fh:\n","        return [json.loads(line) for line in fh.readlines() if line]\n","\n","def get_examples(split):\n","    path = os.path.join(\"data/\", f\"{split}.jsonl\")\n","    examples = read_jsonl(path)\n","\n","    for ex in examples:\n","        ex.update(question=ex[\"question\"] + \"\\n\")\n","        ex.update(answer=ex[\"answer\"] + \"<|endoftext|>\")\n","        ground_truth = delete_extra_zero(ex[\"answer\"].split(\"#### \")[-1].replace(\",\", \"\").replace(\"<|endoftext|>\", \"\"))\n","        ex.update(answer=ex[\"answer\"] + f\" Hence the answer is: {ground_truth}.\" + \"<|endoftext|>\")\n","        ex.update(ground_truth=ground_truth)\n","        \n","\n","    print(f\"Originally {len(examples)} {split} examples\")\n","    return examples\n","\n","\n","\n","def convert_raw_data_to_model_format(tokenizer, max_length, question, answer, gt_answer, configs):\n","    question_start_token, question_end_token, answer_token = configs['question_start_tag'], configs['question_end_tag'], configs['answer_tag']\n","    new_question = question_start_token + question + question_end_token\n","    new_answer = answer_token + answer\n","    full_text = new_question + new_answer\n","    num_question_tokens = len(tokenizer.tokenize(new_question, add_special_tokens=True))\n","\n","    encoded = tokenizer(\n","        full_text, \n","        add_special_tokens=True, \n","        max_length=max_length, \n","        truncation=True, \n","    )\n","    # pad_length = max_length - len(encoded.input_ids)\n","    # pad_input_ids = encoded['input_ids'] \n","    # pad_attention_mask = encoded['attention_mask'] \n","    # label = encoded.input_ids\n","    labels = deepcopy(encoded.input_ids)\n","    encoded['labels'] = labels\n","    encoded['question_length'] = num_question_tokens\n","    encoded['question'] = new_question\n","    encoded['answer'] = new_answer\n","    encoded['gt_answer'] = gt_answer\n","    \n","    \n","    \n","    #change label to -100 for question tokens\n","    for i in range(num_question_tokens): encoded['labels'][i] = -100\n","    return encoded\n","\n","    # return encoded.input_ids, encoded.attention_mask, torch.tensor(label)\n","    \n","\n","\n","\n","class TextDatasetQA(Dataset):\n","    def __init__(self, tokenizer, configs, frac = 1.0, split = None):\n","        super(TextDatasetQA, self).__init__()\n","        self.tokenizer = tokenizer\n","        self.max_length = configs['max_length']\n","        examples = get_examples(split)\n","        N = int(frac * len(examples))\n","        self.data = examples[:N]\n","        self.configs = configs \n","        print(f\"Returning {len(self.data)} {split} examples\")\n","    \n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        question = self.data[idx]['question']\n","        answer = self.data[idx]['answer']\n","        gt_answer = self.data[idx]['ground_truth']  \n","        converted_data = convert_raw_data_to_model_format(\n","            self.tokenizer, self.max_length, question, answer, gt_answer, self.configs)\n","\n","        return converted_data\n","\n","        # pad_input_ids_list = []\n","        # label_list = []\n","        # pad_attention_mask_list = []\n","        # pad_input_ids_list.append(converted_data[0])\n","        # label_list.append(converted_data[1])\n","        # pad_attention_mask_list.append(converted_data[2])\n","        # return encoded\n","        # torch.stack(label_list).squeeze(),\\\n","        # torch.stack(pad_attention_mask_list).squeeze()\n","\n","\n","# def collate_fn(batch):\n","#     input_ids, attention_masks = zip(*batch)\n","#     input_ids = pad_sequence(input_ids, batch_first=True, padding_value=-100)\n","#     attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n","#     return input_ids, attention_masks\n","\n","\n","def get_collate_fn(tokenizer) -> Callable[[List[Dict]], Dict[str, Union[List, torch.Tensor]]]:\n","    \"\"\"Returns a collate function for the given tokenizer.\n","       The collate function takes a list of examples (dicts, where values are lists of\n","       ints [tokens] or strings [the original texts]) and returns a batch of examples,\n","       PyTorch tensors padded to the maximum length. Strings are passed through.\"\"\"\n","    def collate_fn(batch):\n","        # first, pad everything to the same length\n","        padded_batch = {}\n","        for k in batch[0].keys():\n","            if k.endswith('input_ids') or k.endswith('attention_mask') or k.endswith('labels'):\n","                to_pad = [torch.LongTensor(ex[k][::-1]) for ex in batch]\n","                if k.endswith('input_ids'):\n","                    padding_value = tokenizer.pad_token_id\n","                elif k.endswith('attention_mask'):\n","                    padding_value = 0\n","                elif k.endswith('labels'):\n","                    padding_value = -100\n","                else:\n","                    raise ValueError(f\"Unexpected key in batch '{k}'\")\n","                padded_batch[k] = pad_sequence(to_pad, batch_first=True, padding_value=padding_value)\n","                padded_batch[k] = padded_batch[k].flip(dims=[1])\n","            else:\n","                padded_batch[k] = [ex[k] for ex in batch]\n","        # print(f\"Returning: {padded_batch}, {tokenizer.padding_side}\")\n","        return padded_batch\n","    return collate_fn"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Originally 7473 train examples\n","Returning 747 train examples\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(configs['model_id'], padding_side='left')\n","tokenizer.pad_token_id = tokenizer.eos_token_id\n","train_dataset = TextDatasetQA(tokenizer, configs, frac = 0.1, split = \"train\")\n","train_loader = torch.utils.data.DataLoader(\n","    train_dataset, batch_size=1, shuffle=True, collate_fn=get_collate_fn(tokenizer))\n","a = next(train_loader.__iter__())"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def find_all_linear_names(model):\n","    cls = torch.nn.Linear\n","    lora_module_names = set()\n","    for name, module in model.named_modules():\n","        if isinstance(module, cls):\n","            names = name.split('.')\n","            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n","    if 'lm_head' in lora_module_names: # needed for 16-bit\n","        lora_module_names.remove('lm_head')\n","    return list(lora_module_names)"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[],"source":["def get_batch_logps(logits: torch.FloatTensor, labels: torch.LongTensor, average_log_prob: bool = False) -> torch.FloatTensor:\n","    \"\"\"Compute the log probabilities of the given labels under the given logits.\n","\n","    Args:\n","        logits: Logits of the model (unnormalized). Shape: (batch_size, sequence_length, vocab_size)\n","        labels: Labels for which to compute the log probabilities. Label tokens with a value of -100 are ignored. Shape: (batch_size, sequence_length)\n","        average_log_prob: If True, return the average log probability per (non-masked) token. Otherwise, return the sum of the log probabilities of the (non-masked) tokens.\n","\n","    Returns:\n","        A tensor of shape (batch_size,) containing the average/sum log probabilities of the given labels under the given logits.\n","    \"\"\"\n","    assert logits.shape[:-1] == labels.shape\n","\n","    labels = labels[:, 1:].clone()\n","    logits = logits[:, :-1, :]\n","    loss_mask = (labels != -100)\n","\n","    # dummy token; we'll ignore the losses on these tokens later\n","    labels[labels == -100] = 0\n","\n","    per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)\n","\n","    if average_log_prob:\n","        return (per_token_logps * loss_mask).sum(-1) / loss_mask.sum(-1)\n","    else:\n","        return (per_token_logps * loss_mask).sum(-1)"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[],"source":["def print_trainable_parameters(model):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model.\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for _, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(\n","        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@hydra.main(version_base=None, config_path=\"\", config_name=\"finetune_config\")\n","def main(cfg):\n","    if os.environ.get('LOCAL_RANK') is not None:\n","        local_rank = int(os.environ.get('LOCAL_RANK', '0'))\n","    os.environ[\"WANDB_DISABLED\"] = \"true\"\n","    Path(configs['save_dir']).mkdir(parents=True, exist_ok=True)\n","    \n","    tokenizer = AutoTokenizer.from_pretrained(configs['model_id'], padding_side='left')\n","    tokenizer.pad_token = tokenizer.eos_token\n","    \n","    batch_size = configs['batch_size']\n","    train_dataset = TextDatasetQA(tokenizer, configs, frac = 0.1, split = \"train\")\n","    train_loader = torch.utils.data.DataLoader(\n","        train_dataset, batch_size=batch_size, shuffle=True, collate_fn=get_collate_fn(tokenizer))\n","    test_dataset = TextDatasetQA(tokenizer, configs, frac = 0.1, split = \"test\")\n","    test_loader = torch.utils.data.DataLoader(\n","        test_dataset, batch_size=batch_size, shuffle=False, collate_fn=get_collate_fn(tokenizer))\n","\n","\n","    gradient_accumulation_steps = configs['gradient_accumulation_steps']\n","\n","    # --nproc_per_node gives the number of GPUs per = num_devices. take it from torchrun/os.environ\n","    num_devices = int(os.environ.get('WORLD_SIZE', 1))\n","    print(f\"num_devices: {num_devices}\")\n","    \n","    max_steps = int(cfg.num_epochs*len(train_dataset))//(batch_size*gradient_accumulation_steps*num_devices)\n","    print(f\"max_steps: {max_steps}\")\n","    steps_per_epoch = len(train_dataset)//(batch_size*gradient_accumulation_steps*num_devices)\n","    \n","    training_args = transformers.TrainingArguments(\n","            per_device_train_batch_size=batch_size,\n","            per_device_eval_batch_size=batch_size,\n","            gradient_accumulation_steps=gradient_accumulation_steps,\n","            warmup_steps=max(1, max_steps//10),\n","            max_steps=max_steps,\n","            learning_rate=configs['lr'],\n","            bf16=True,\n","            bf16_full_eval=True,\n","            logging_steps=max(1,max_steps//20),\n","            logging_dir=f'{configs['save_dir']}/logs',\n","            output_dir=configs['save_dir'],\n","            optim=\"paged_adamw_32bit\",\n","            save_steps=steps_per_epoch,\n","            save_only_model=True,\n","            ddp_find_unused_parameters= False,\n","            evaluation_strategy=\"no\",\n","            deepspeed='config/ds_config.json',\n","            weight_decay = cfg.weight_decay\n","        )\n","    model = AutoModelForCausalLM.from_pretrained(model_id, use_flash_attention_2=model_cfg[\"flash_attention2\"]==\"true\", torch_dtype=torch.bfloat16, trust_remote_code = True)\n","    \n","    # Hot fix for https://discuss.huggingface.co/t/help-with-llama-2-finetuning-setup/50035\n","    model.generation_config.do_sample = True\n","    \n","    if configs[\"gradient_checkpointing\"] == \"true\":\n","        model.gradient_checkpointing_enable()\n","    \n","    config = LoraConfig(\n","        r=cfg.LoRA.r, \n","        lora_alpha=cfg.LoRA.alpha, \n","        target_modules=find_all_linear_names(model), \n","        lora_dropout=cfg.LoRA.dropout,\n","        bias=\"none\", \n","        task_type=\"CAUSAL_LM\"\n","    )\n","    if cfg.LoRA.r != 0:\n","        model = get_peft_model(model, config)\n","    \n","    trainer = CustomTrainer(\n","        model=model,\n","        train_dataset=torch_format_dataset,\n","        eval_dataset=torch_format_dataset,\n","        args=training_args,\n","        data_collator=custom_data_collator,\n","    )\n","    model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n","    trainer.train()\n","\n","    #save the model\n","    if cfg.LoRA.r != 0:\n","        model = model.merge_and_unload()\n","    \n","    model.save_pretrained(cfg.save_dir)\n","    tokenizer.save_pretrained(cfg.save_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":2}
