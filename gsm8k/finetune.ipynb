{"cells":[{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","import hydra \n","import transformers\n","import os\n","from peft import LoraConfig, get_peft_model\n","from pathlib import Path\n","from omegaconf import OmegaConf\n","import json\n","import transformers\n","import torch\n","import os\n","import json\n","import random\n","import numpy as np\n","import argparse\n","from typing import Dict, List, Optional, Iterator, Callable, Union, Tuple\n","import datasets\n","from torch.nn.utils.rnn import pad_sequence\n","from tqdm import tqdm\n","import sys\n","import re\n","from rouge_score import rouge_scorer\n","from collections import defaultdict\n","import re\n","from torch.utils.data import Dataset"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["configs = {\n","    'question_start_tag': \"[INST] \",\n","    \"question_end_tag\": \" End your answer as, 'Hence the answer is: '. [/INST]\",\n","    \"answer_tag\": \"\",\n","    'max_length': 512,\n","    'model_id': \"mistralai/Mistral-7B-Instruct-v0.1\",\n","    'batch_size': 64\n","}"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["def delete_extra_zero(n):\n","    try:\n","        n=float(n)\n","    except:\n","        # print(\"None {}\".format(n))\n","        return n\n","    if isinstance(n, int):\n","        return str(n)\n","    if isinstance(n, float):\n","        n = str(n).rstrip('0')  \n","        n = int(n.rstrip('.')) if n.endswith('.') else float(n)  \n","        n=str(n)\n","        return n\n","    \n","    \n","def read_jsonl(path: str):\n","    with open(path) as fh:\n","        return [json.loads(line) for line in fh.readlines() if line]\n","\n","def get_examples(split):\n","    path = os.path.join(\"data/\", f\"{split}.jsonl\")\n","    examples = read_jsonl(path)\n","\n","    for ex in examples:\n","        ex.update(question=ex[\"question\"] + \"\\n\")\n","        ex.update(answer=ex[\"answer\"] + \"<|endoftext|>\")\n","        ground_truth = delete_extra_zero(ex[\"answer\"].split(\"#### \")[-1].replace(\",\", \"\").replace(\"<|endoftext|>\", \"\"))\n","        ex.update(answer=ex[\"answer\"] + f\" Hence the answer is: {ground_truth}.\" + \"<|endoftext|>\")\n","        \n","\n","    print(f\"Originally {len(examples)} {split} examples\")\n","    return examples\n","\n","\n","\n","def convert_raw_data_to_model_format(tokenizer, max_length, question, answer, configs):\n","    question_start_token, question_end_token, answer_token = configs['question_start_tag'], configs['question_end_tag'], configs['answer_tag']\n","    new_question = question_start_token + question + question_end_token\n","    new_answer = answer_token + answer\n","    full_text = new_question + new_answer\n","    num_question_tokens = len(tokenizer.tokenize(new_question, add_special_tokens=True))\n","\n","    encoded = tokenizer(\n","        full_text, \n","        add_special_tokens=True, \n","        max_length=max_length, \n","        truncation=True, \n","    )\n","    # pad_length = max_length - len(encoded.input_ids)\n","    # pad_input_ids = encoded['input_ids'] \n","    # pad_attention_mask = encoded['attention_mask'] \n","    # label = encoded.input_ids\n","    labels = encoded.input_ids\n","    encoded['labels'] = labels\n","    \n","    #change label to -100 for question tokens\n","    for i in range(num_question_tokens): encoded['labels'][i] = -100\n","    return encoded\n","\n","    # return encoded.input_ids, encoded.attention_mask, torch.tensor(label)\n","    \n","\n","\n","\n","class TextDatasetQA(Dataset):\n","    def __init__(self, tokenizer, configs, frac = 1.0, split = None):\n","        super(TextDatasetQA, self).__init__()\n","        self.tokenizer = tokenizer\n","        self.max_length = configs['max_length']\n","        examples = get_examples(split)\n","        N = int(frac * len(examples))\n","        self.data = examples[:N]\n","        self.configs = configs \n","        print(f\"Returning {len(self.data)} {split} examples\")\n","    \n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        question = self.data[idx]['question']\n","        answer = self.data[idx]['answer']\n","        converted_data = convert_raw_data_to_model_format(\n","            self.tokenizer, self.max_length, question, answer, self.configs)\n","\n","        return converted_data\n","\n","        # pad_input_ids_list = []\n","        # label_list = []\n","        # pad_attention_mask_list = []\n","        # pad_input_ids_list.append(converted_data[0])\n","        # label_list.append(converted_data[1])\n","        # pad_attention_mask_list.append(converted_data[2])\n","        # return encoded\n","        # torch.stack(label_list).squeeze(),\\\n","        # torch.stack(pad_attention_mask_list).squeeze()\n","\n","\n","# def collate_fn(batch):\n","#     input_ids, attention_masks = zip(*batch)\n","#     input_ids = pad_sequence(input_ids, batch_first=True, padding_value=-100)\n","#     attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n","#     return input_ids, attention_masks\n","\n","\n","def get_collate_fn(tokenizer) -> Callable[[List[Dict]], Dict[str, Union[List, torch.Tensor]]]:\n","    \"\"\"Returns a collate function for the given tokenizer.\n","       The collate function takes a list of examples (dicts, where values are lists of\n","       ints [tokens] or strings [the original texts]) and returns a batch of examples,\n","       PyTorch tensors padded to the maximum length. Strings are passed through.\"\"\"\n","    def collate_fn(batch):\n","        # first, pad everything to the same length\n","        padded_batch = {}\n","        for k in batch[0].keys():\n","            if k.endswith('input_ids') or k.endswith('attention_mask') or k.endswith('labels'):\n","                to_pad = [torch.LongTensor(ex[k][::-1]) for ex in batch]\n","                if k.endswith('input_ids'):\n","                    padding_value = tokenizer.pad_token_id\n","                elif k.endswith('attention_mask'):\n","                    padding_value = 0\n","                elif k.endswith('labels'):\n","                    padding_value = -100\n","                else:\n","                    raise ValueError(f\"Unexpected key in batch '{k}'\")\n","                padded_batch[k] = pad_sequence(to_pad, batch_first=True, padding_value=padding_value)\n","                padded_batch[k] = padded_batch[k].flip(dims=[1])\n","            else:\n","                padded_batch[k] = [ex[k] for ex in batch]\n","        # print(f\"Returning: {padded_batch}, {tokenizer.padding_side}\")\n","        return padded_batch\n","    return collate_fn"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Originally 7473 train examples\n","Returning 747 train examples\n"]},{"data":{"text/plain":["tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n","          -100,  -100,  -100,  -100, 28798,   283,   365,   659, 28705, 28750,\n","         28783,   648, 28705, 28740, 28750,   327,  2087, 28750, 28783, 28806,\n","         28740, 28750, 28746, 28781, 28734,  4060, 28781, 28734,  1829,  9343,\n","         28723,    13, 28798,   283,   334,   659, 28705, 28750,  1318, 28705,\n","         28781, 28734,   327,  2087, 28750, 28736, 28781, 28734, 28746, 28783,\n","         28734,  4060, 28783, 28734,  1829,  9343, 28723,    13,  5816,   994,\n","         28725,   736,   460, 28705, 28750, 28783,   648, 28705, 28781, 28734,\n","           648, 28705, 28783, 28734,   327,  2087, 28750, 28783, 28806, 28781,\n","         28734, 28806, 28783, 28734, 28746, 28740, 28781, 28783,  4060, 28740,\n","         28781, 28783,  1829,  9343, 21324, 28723,    13,  2000, 28705, 28740,\n","         28781, 28783, 28789, 28766,   416,  1009,   772, 28766, 28767, 12478,\n","           272,  4372,   349, 28747, 28705, 28740, 28781, 28783, 26364, 28766,\n","           416,  1009,   772, 28766, 28767]])"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["tokenizer = AutoTokenizer.from_pretrained(configs['model_id'], padding_side='left')\n","tokenizer.pad_token_id = tokenizer.eos_token_id\n","train_dataset = TextDatasetQA(tokenizer, configs, frac = 0.1, split = \"train\")\n","train_loader = torch.utils.data.DataLoader(\n","    train_dataset, batch_size=1, shuffle=True, collate_fn=get_collate_fn(tokenizer))\n","a = next(train_loader.__iter__())\n","a['input_ids']\n","# next(train_loader.__iter__())['attention_mask']"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["def find_all_linear_names(model):\n","    cls = torch.nn.Linear\n","    lora_module_names = set()\n","    for name, module in model.named_modules():\n","        if isinstance(module, cls):\n","            names = name.split('.')\n","            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n","    if 'lm_head' in lora_module_names: # needed for 16-bit\n","        lora_module_names.remove('lm_head')\n","    return list(lora_module_names)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["7473 train examples\n","1319 test examples\n"]},{"data":{"text/plain":["[{'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\\n',\n","  'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72<|endoftext|>'},\n"," {'question': 'Weng earns $12 an hour for babysitting. Yesterday, she just did 50 minutes of babysitting. How much did she earn?\\n',\n","  'answer': 'Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute.\\nWorking 50 minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10.\\n#### 10<|endoftext|>'}]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["def _get_batch_logps(logits: torch.FloatTensor, labels: torch.LongTensor, average_log_prob: bool = False) -> torch.FloatTensor:\n","    \"\"\"Compute the log probabilities of the given labels under the given logits.\n","\n","    Args:\n","        logits: Logits of the model (unnormalized). Shape: (batch_size, sequence_length, vocab_size)\n","        labels: Labels for which to compute the log probabilities. Label tokens with a value of -100 are ignored. Shape: (batch_size, sequence_length)\n","        average_log_prob: If True, return the average log probability per (non-masked) token. Otherwise, return the sum of the log probabilities of the (non-masked) tokens.\n","\n","    Returns:\n","        A tensor of shape (batch_size,) containing the average/sum log probabilities of the given labels under the given logits.\n","    \"\"\"\n","    assert logits.shape[:-1] == labels.shape\n","\n","    labels = labels[:, 1:].clone()\n","    logits = logits[:, :-1, :]\n","    loss_mask = (labels != -100)\n","\n","    # dummy token; we'll ignore the losses on these tokens later\n","    labels[labels == -100] = 0\n","\n","    per_token_logps = torch.gather(logits.log_softmax(-1), dim=2, index=labels.unsqueeze(2)).squeeze(2)\n","\n","    if average_log_prob:\n","        return (per_token_logps * loss_mask).sum(-1) / loss_mask.sum(-1)\n","    else:\n","        return (per_token_logps * loss_mask).sum(-1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def print_trainable_parameters(model):\n","    \"\"\"\n","    Prints the number of trainable parameters in the model.\n","    \"\"\"\n","    trainable_params = 0\n","    all_param = 0\n","    for _, param in model.named_parameters():\n","        all_param += param.numel()\n","        if param.requires_grad:\n","            trainable_params += param.numel()\n","    print(\n","        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["@hydra.main(version_base=None, config_path=\"config\", config_name=\"finetune\")\n","def main(cfg):\n","    if os.environ.get('LOCAL_RANK') is not None:\n","        local_rank = int(os.environ.get('LOCAL_RANK', '0'))\n","        device_map = {'': local_rank}\n","    os.environ[\"WANDB_DISABLED\"] = \"true\"\n","    model_cfg = get_model_identifiers_from_yaml(cfg.model_family)\n","    model_id = model_cfg[\"hf_key\"]\n","    Path(cfg.save_dir).mkdir(parents=True, exist_ok=True)\n","    # save the cfg file\n","    #if master process\n","    if os.environ.get('LOCAL_RANK') is None or local_rank == 0:\n","        with open(f'{cfg.save_dir}/cfg.yaml', 'w') as f:\n","            OmegaConf.save(cfg, f)\n","    tokenizer = AutoTokenizer.from_pretrained(model_id)\n","    tokenizer.pad_token = tokenizer.eos_token\n","    max_length = 500\n","    \n","    get_examples('train')\n","    \n","    torch_format_dataset = TextDatasetQA(cfg.data_path, tokenizer=tokenizer, model_family = cfg.model_family, max_length=max_length, split=cfg.split)\n","    batch_size = cfg.batch_size\n","    gradient_accumulation_steps = cfg.gradient_accumulation_steps\n","    # --nproc_per_node gives the number of GPUs per = num_devices. take it from torchrun/os.environ\n","    num_devices = int(os.environ.get('WORLD_SIZE', 1))\n","    print(f\"num_devices: {num_devices}\")\n","    \n","    max_steps = int(cfg.num_epochs*len(torch_format_dataset))//(batch_size*gradient_accumulation_steps*num_devices)\n","    print(f\"max_steps: {max_steps}\")\n","    if cfg.split == \"full\":\n","        steps_per_epoch = len(torch_format_dataset)//(batch_size*gradient_accumulation_steps*num_devices)\n","    else:\n","        #dont save retain model ckpt at every epoch\n","        steps_per_epoch = max_steps\n","    \n","    training_args = transformers.TrainingArguments(\n","            per_device_train_batch_size=batch_size,\n","            per_device_eval_batch_size=batch_size,\n","            gradient_accumulation_steps=gradient_accumulation_steps,\n","            warmup_steps=max(1, max_steps//10),\n","            max_steps=max_steps,\n","            learning_rate=cfg.lr,\n","            bf16=True,\n","            bf16_full_eval=True,\n","            logging_steps=max(1,max_steps//20),\n","            logging_dir=f'{cfg.save_dir}/logs',\n","            output_dir=cfg.save_dir,\n","            optim=\"paged_adamw_32bit\",\n","            save_steps=steps_per_epoch,\n","            save_only_model=True,\n","            ddp_find_unused_parameters= False,\n","            evaluation_strategy=\"no\",\n","            deepspeed='config/ds_config.json',\n","            weight_decay = cfg.weight_decay\n","        )\n","    model = AutoModelForCausalLM.from_pretrained(model_id, use_flash_attention_2=model_cfg[\"flash_attention2\"]==\"true\", torch_dtype=torch.bfloat16, trust_remote_code = True)\n","    \n","    # Hot fix for https://discuss.huggingface.co/t/help-with-llama-2-finetuning-setup/50035\n","    model.generation_config.do_sample = True\n","    \n","    if model_cfg[\"gradient_checkpointing\"] == \"true\":\n","        model.gradient_checkpointing_enable()\n","    \n","    config = LoraConfig(\n","        r=cfg.LoRA.r, \n","        lora_alpha=cfg.LoRA.alpha, \n","        target_modules=find_all_linear_names(model), \n","        lora_dropout=cfg.LoRA.dropout,\n","        bias=\"none\", \n","        task_type=\"CAUSAL_LM\"\n","    )\n","    if cfg.LoRA.r != 0:\n","        model = get_peft_model(model, config)\n","    \n","    trainer = CustomTrainer(\n","        model=model,\n","        train_dataset=torch_format_dataset,\n","        eval_dataset=torch_format_dataset,\n","        args=training_args,\n","        data_collator=custom_data_collator,\n","    )\n","    model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n","    trainer.train()\n","\n","    #save the model\n","    if cfg.LoRA.r != 0:\n","        model = model.merge_and_unload()\n","    \n","    model.save_pretrained(cfg.save_dir)\n","    tokenizer.save_pretrained(cfg.save_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if __name__ == \"__main__\":\n","    main()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":2}
